
@article{soyez_pileup_2019,
	title = {Pileup mitigation at the {LHC}: a theorist's view},
	volume = {803},
	issn = {03701573},
	shorttitle = {Pileup mitigation at the {LHC}},
	url = {http://arxiv.org/abs/1801.09721},
	doi = {10.1016/j.physrep.2019.01.007},
	abstract = {To maximise the potential for new measurements and discoveries at the LHC, the machine delivers as high as possible collision rates. As a consequence, multiple proton-proton collisions occur whenever two bunches cross. Interesting high-energy (hard) collisions are therefore contaminated by several soft, zero-bias, ones. This effect known as pileup pollutes the final state of the collision. It complicates the reconstruction of the objects in this final state, resulting in increased experimental uncertainties. To reduce these uncertainties, and improve the quality and precision of LHC measurements, techniques are devised to correct for the effects of pileup. This document provides a theoretical review of the main methods used during Run I and II of the LHC to mitigate pileup effects. I start with an in-depth presentation of the area--median used for the majority of applications, including several refinements of the original idea, their practical implementation and an assessment of their efficiency and robustness. I then focus on several theoretical calculations that can provide both quantitative and qualitative information on the area--median approach. In the case of boosted jets, a field that has seen a wide interest recently, a set of methods, known as grooming techniques has also been used. I describe these techniques, address their performance and briefly show that they are amenable to a theoretical, analytic, understanding. The last part of this review focuses on ideas oriented towards future pileup mitigation techniques. This includes new methods that have recently been proposed as well as a large series of alternative ideas. The latter are yet unpublished and have not received the same amount of investigation than the former but they have the potential to bring new developments and further improvement over existing techniques in a future where pileup mitigation will be crucial.},
	urldate = {2023-12-18},
	journal = {Physics Reports},
	author = {Soyez, Gregory},
	month = apr,
	year = {2019},
	note = {arXiv:1801.09721 [hep-ex, physics:hep-ph]},
	keywords = {High Energy Physics - Experiment, High Energy Physics - Phenomenology},
	pages = {1--158},
	annote = {Comment: 259 pages, 101 figures. Version to appear in Phys. Rept},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\TF9TWEFR\\1801.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\HBQJ5JEJ\\Soyez - 2019 - Pileup mitigation at the LHC a theorist's view.pdf:application/pdf},
}

@misc{noauthor_how_nodate,
	title = {How {CMS} weeds out particles that pile up {\textbar} {CMS} {Experiment}},
	url = {https://cms.cern/news/how-cms-weeds-out-particles-pile},
	urldate = {2023-12-18},
	file = {How CMS weeds out particles that pile up | CMS Experiment:C\:\\Users\\David\\Zotero\\storage\\6HJYCPZX\\how-cms-weeds-out-particles-pile.html:text/html},
}

@incollection{rieger_towards_2019,
	title = {Towards {Real}-{Time} {Head} {Pose} {Estimation}: {Exploring} {Parameter}-{Reduced} {Residual} {Networks} on {In}-the-wild {Datasets}},
	volume = {11606},
	shorttitle = {Towards {Real}-{Time} {Head} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1906.05203},
	abstract = {Head poses are a key component of human bodily communication and thus a decisive element of human-computer interaction. Real-time head pose estimation is crucial in the context of human-robot interaction or driver assistance systems. The most promising approaches for head pose estimation are based on Convolutional Neural Networks (CNNs). However, CNN models are often too complex to achieve real-time performance. To face this challenge, we explore a popular subgroup of CNNs, the Residual Networks (ResNets) and modify them in order to reduce their number of parameters. The ResNets are modifed for different image sizes including low-resolution images and combined with a varying number of layers. They are trained on in-the-wild datasets to ensure real-world applicability. As a result, we demonstrate that the performance of the ResNets can be maintained while reducing the number of parameters. The modified ResNets achieve state-of-the-art accuracy and provide fast inference for real-time applicability.},
	urldate = {2023-11-29},
	author = {Rieger, Ines and Hauenstein, Thomas and Hettenkofer, Sebastian and Garbas, Jens-Uwe},
	year = {2019},
	doi = {10.1007/978-3-030-22999-3_12},
	note = {arXiv:1906.05203 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {123--134},
	annote = {Comment: 32nd International Conference on Industrial, Engineering \& Other Applications of Applied Intelligent Systems (IEA/AIE 2019)},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\TLFZBURU\\1906.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\48IE3DMJ\\Rieger et al. - 2019 - Towards Real-Time Head Pose Estimation Exploring .pdf:application/pdf},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\4VKWJNPW\\1512.html:text/html},
}

@misc{noauthor_what_nodate,
	title = {What is the significance of the {Reg}\_Max parameter? {How} to adjust its value? · {Issue} \#3072 · ultralytics/ultralytics},
	shorttitle = {What is the significance of the {Reg}\_Max parameter?},
	url = {https://github.com/ultralytics/ultralytics/issues/3072},
	abstract = {Search before asking I have searched the YOLOv8 issues and discussions and found no similar questions. Question Hello, I want to know the meaning of Reg\_Max and how to adjust this parameter. I look...},
	language = {en},
	urldate = {2023-11-28},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\IS9IQMPW\\3072.html:text/html},
}

@misc{noauthor_kerascv_2023,
	title = {{KerasCV}},
	url = {https://github.com/keras-team/keras-cv},
	abstract = {Industry-strength Computer Vision workflows with Keras},
	urldate = {2023-11-28},
	publisher = {Keras},
	month = nov,
	year = {2023},
	note = {original-date: 2020-05-18T22:39:21Z},
}

@article{wang_cspnet_2019,
	title = {{CSPNet}: {A} {New} {Backbone} that can {Enhance} {Learning} {Capability} of {CNN}},
	shorttitle = {{CSPNet}},
	url = {http://arxiv.org/abs/1911.11929},
	abstract = {Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. We attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20\% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms state-of-the-art approaches in terms of AP50 on the MS COCO object detection dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet. Source code is at https://github.com/WongKinYiu/CrossStagePartialNetworks.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Wang, Chien-Yao and Liao, Hong-Yuan Mark and Yeh, I.-Hau and Wu, Yueh-Hua and Chen, Ping-Yang and Hsieh, Jun-Wei},
	month = nov,
	year = {2019},
	note = {arXiv:1911.11929 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\U7DTITY9\\1911.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\KTTICY34\\Wang et al. - 2019 - CSPNet A New Backbone that can Enhance Learning C.pdf:application/pdf},
}

@article{terven_comprehensive_2023,
	title = {A {Comprehensive} {Review} of {YOLO}: {From} {YOLOv1} and {Beyond}},
	shorttitle = {A {Comprehensive} {Review} of {YOLO}},
	url = {http://arxiv.org/abs/2304.00501},
	abstract = {YOLO has become a central real-time object detection system for robotics, driverless cars, and video monitoring applications. We present a comprehensive analysis of YOLO's evolution, examining the innovations and contributions in each iteration from the original YOLO up to YOLOv8, YOLO-NAS, and YOLO with Transformers. We start by describing the standard metrics and postprocessing; then, we discuss the major changes in network architecture and training tricks for each model. Finally, we summarize the essential lessons from YOLO's development and provide a perspective on its future, highlighting potential research directions to enhance real-time object detection systems.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Terven, Juan and Cordova-Esparza, Diana},
	month = oct,
	year = {2023},
	note = {arXiv:2304.00501 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.2.10},
	annote = {Comment: 34 pages, 19 figures, 4 tables, submitted to ACM Computing Surveys. This version adds information about YOLO with transformers},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\K36E3UGG\\2304.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\L3ULRR62\\Terven and Cordova-Esparza - 2023 - A Comprehensive Review of YOLO From YOLOv1 and Be.pdf:application/pdf},
}

@article{wang_yolov7_2022,
	title = {{YOLOv7}: {Trainable} bag-of-freebies sets new state-of-the-art for real-time object detectors},
	shorttitle = {{YOLOv7}},
	url = {http://arxiv.org/abs/2207.02696},
	abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/WongKinYiu/yolov7.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
	month = jul,
	year = {2022},
	note = {arXiv:2207.02696 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\6JW9QWED\\2207.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\RHA6QCFR\\Wang et al. - 2022 - YOLOv7 Trainable bag-of-freebies sets new state-o.pdf:application/pdf},
}

@article{li_yolov6_2022,
	title = {{YOLOv6}: {A} {Single}-{Stage} {Object} {Detection} {Framework} for {Industrial} {Applications}},
	shorttitle = {{YOLOv6}},
	url = {http://arxiv.org/abs/2209.02976},
	abstract = {For years, the YOLO series has been the de facto industry-level standard for efficient object detection. The YOLO community has prospered overwhelmingly to enrich its use in a multitude of hardware platforms and abundant scenarios. In this technical report, we strive to push its limits to the next level, stepping forward with an unwavering mindset for industry application. Considering the diverse requirements for speed and accuracy in the real environment, we extensively examine the up-to-date object detection advancements either from industry or academia. Specifically, we heavily assimilate ideas from recent network design, training strategies, testing techniques, quantization, and optimization methods. On top of this, we integrate our thoughts and practice to build a suite of deployment-ready networks at various scales to accommodate diversified use cases. With the generous permission of YOLO authors, we name it YOLOv6. We also express our warm welcome to users and contributors for further enhancement. For a glimpse of performance, our YOLOv6-N hits 35.9\% AP on the COCO dataset at a throughput of 1234 FPS on an NVIDIA Tesla T4 GPU. YOLOv6-S strikes 43.5\% AP at 495 FPS, outperforming other mainstream detectors at the same scale{\textasciitilde}(YOLOv5-S, YOLOX-S, and PPYOLOE-S). Our quantized version of YOLOv6-S even brings a new state-of-the-art 43.3\% AP at 869 FPS. Furthermore, YOLOv6-M/L also achieves better accuracy performance (i.e., 49.5\%/52.3\%) than other detectors with a similar inference speed. We carefully conducted experiments to validate the effectiveness of each component. Our code is made available at https://github.com/meituan/YOLOv6.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Li, Chuyi and Li, Lulu and Jiang, Hongliang and Weng, Kaiheng and Geng, Yifei and Li, Liang and Ke, Zaidan and Li, Qingyuan and Cheng, Meng and Nie, Weiqiang and Li, Yiduo and Zhang, Bo and Liang, Yufei and Zhou, Linyuan and Xu, Xiaoming and Chu, Xiangxiang and Wei, Xiaoming and Wei, Xiaolin},
	month = sep,
	year = {2022},
	note = {arXiv:2209.02976 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: technical report},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\ETJ2AZUP\\2209.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\8GM6W7GZ\\Li et al. - 2022 - YOLOv6 A Single-Stage Object Detection Framework .pdf:application/pdf},
}

@article{bochkovskiy_yolov4_2020,
	title = {{YOLOv4}: {Optimal} {Speed} and {Accuracy} of {Object} {Detection}},
	shorttitle = {{YOLOv4}},
	url = {http://arxiv.org/abs/2004.10934},
	abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of {\textasciitilde}65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
	month = apr,
	year = {2020},
	note = {arXiv:2004.10934 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\4F22753Y\\2004.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\AXCKH4VC\\Bochkovskiy et al. - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detec.pdf:application/pdf},
}

@incollection{he_spatial_2014,
	title = {Spatial {Pyramid} {Pooling} in {Deep} {Convolutional} {Networks} for {Visual} {Recognition}},
	volume = {8691},
	url = {http://arxiv.org/abs/1406.4729},
	abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image. This requirement is "artificial" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102x faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank \#2 in object detection and \#3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.},
	urldate = {2023-11-27},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2014},
	doi = {10.1007/978-3-319-10578-9_23},
	note = {arXiv:1406.4729 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {346--361},
	annote = {Comment: This manuscript is the accepted version for IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2015. See Changelog},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\RW3N3DUE\\1406.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\BV3MEF8J\\He et al. - 2014 - Spatial Pyramid Pooling in Deep Convolutional Netw.pdf:application/pdf},
}

@article{redmon_yolo9000_2016,
	title = {{YOLO9000}: {Better}, {Faster}, {Stronger}},
	shorttitle = {{YOLO9000}},
	url = {http://arxiv.org/abs/1612.08242},
	abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = dec,
	year = {2016},
	note = {arXiv:1612.08242 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\PLZ6ICLT\\1612.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\P35VW2GC\\Redmon and Farhadi - 2016 - YOLO9000 Better, Faster, Stronger.pdf:application/pdf},
}

@misc{ultralytics_home_nodate,
	title = {Home},
	url = {https://docs.ultralytics.com/},
	abstract = {Explore a complete guide to Ultralytics YOLOv8, a high-speed, high-accuracy object detection \& image segmentation model. Installation, prediction, training tutorials and more.},
	language = {en},
	urldate = {2023-11-22},
	author = {Ultralytics},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\KH5XVQS8\\docs.ultralytics.com.html:text/html},
}

@article{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://arxiv.org/abs/1506.02640},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = may,
	year = {2016},
	note = {arXiv:1506.02640 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\4R9RN94W\\1506.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\DL685QR5\\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf},
}

@article{fleuret_deep_nodate,
	title = {Deep learning 4.5. {Pooling}},
	language = {en},
	journal = {Deep learning},
	author = {Fleuret, Francois},
	file = {Fleuret - Deep learning 4.5. Pooling.pdf:C\:\\Users\\David\\Zotero\\storage\\RX3QSCP9\\Fleuret - Deep learning 4.5. Pooling.pdf:application/pdf},
}

@article{ramachandran_searching_2017,
	title = {Searching for {Activation} {Functions}},
	url = {http://arxiv.org/abs/1710.05941},
	abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, \$f(x) = x {\textbackslash}cdot {\textbackslash}text\{sigmoid\}({\textbackslash}beta x)\$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9{\textbackslash}\% for Mobile NASNet-A and 0.6{\textbackslash}\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
	urldate = {2023-11-20},
	publisher = {arXiv},
	author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
	month = oct,
	year = {2017},
	note = {arXiv:1710.05941 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Updated version of "Swish: a Self-Gated Activation Function"},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\H3CDMLEK\\1710.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\QMN6HU5L\\Ramachandran et al. - 2017 - Searching for Activation Functions.pdf:application/pdf},
}

@article{dubey_activation_2022,
	title = {Activation {Functions} in {Deep} {Learning}: {A} {Comprehensive} {Survey} and {Benchmark}},
	shorttitle = {Activation {Functions} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2109.14545},
	abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: {\textbackslash}url\{https://github.com/shivram1987/ActivationFunctions\}.},
	urldate = {2023-11-20},
	publisher = {arXiv},
	author = {Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
	month = jun,
	year = {2022},
	note = {arXiv:2109.14545 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Accepted in Neurocomputing, Elsevier},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\GBHKD469\\2109.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\LSSNJ9PU\\Dubey et al. - 2022 - Activation Functions in Deep Learning A Comprehen.pdf:application/pdf},
}

@article{hendrycks_gaussian_2023,
	title = {Gaussian {Error} {Linear} {Units} ({GELUs})},
	url = {http://arxiv.org/abs/1606.08415},
	abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x{\textbackslash}Phi(x)\$, where \${\textbackslash}Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x{\textbackslash}mathbf\{1\}\_\{x{\textgreater}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
	urldate = {2023-11-20},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	month = jun,
	year = {2023},
	note = {arXiv:1606.08415 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Trimmed version of 2016 draft},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\RMYTWA6X\\1606.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\QS8K77JE\\Hendrycks and Gimpel - 2023 - Gaussian Error Linear Units (GELUs).pdf:application/pdf},
}

@article{fleuret_deep_nodate-1,
	title = {Deep learning 4.4. {Convolutions}},
	language = {en},
	journal = {Deep learning},
	author = {Fleuret, Francois},
	file = {Fleuret - Deep learning 4.4. Convolutions.pdf:C\:\\Users\\David\\Zotero\\storage\\Q33X2P5D\\Fleuret - Deep learning 4.4. Convolutions.pdf:application/pdf},
}

@misc{noauthor_algorithme_2023,
	title = {Algorithme du gradient},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://fr.wikipedia.org/w/index.php?title=Algorithme_du_gradient&oldid=207455034},
	abstract = {L'algorithme du gradient, aussi appelé algorithme de descente de gradient, désigne un algorithme d'optimisation différentiable. Il est par conséquent destiné à minimiser une fonction réelle différentiable définie sur un espace euclidien (par exemple, 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    \{{\textbackslash}displaystyle {\textbackslash}mathbb \{R\} {\textasciicircum}\{n\}\}
  , l'espace des n-uplets de nombres réels, muni d'un produit scalaire) ou, plus généralement, sur un espace hilbertien. L'algorithme est itératif et procède donc par améliorations successives. Au point courant, un déplacement est effectué dans la direction opposée au gradient, de manière à faire décroître la fonction. Le déplacement le long de cette direction est déterminé par la technique numérique connue sous le nom de recherche linéaire. Cette description montre que l'algorithme fait partie de la famille des algorithmes à directions de descente.
Les algorithmes d'optimisation sont généralement écrits pour minimiser une fonction. Si l'on désire maximiser une fonction, il suffira de minimiser son opposée.
Il est important de garder à l'esprit le fait que le gradient, et donc la direction de déplacement, dépend du produit scalaire qui équipe l'espace hilbertien ; l'efficacité de l'algorithme dépend donc de ce produit scalaire.
L'algorithme du gradient est également connu sous le nom d'algorithme de la plus forte pente ou de la plus profonde descente (steepest descent, en anglais) parce que le gradient est la pente de la fonction linéarisée au point courant et est donc, localement, sa plus forte pente (notion qui dépend du produit scalaire).
Dans sa version la plus simple, l'algorithme ne permet de trouver ou d'approcher qu'un point stationnaire (i.e., un point en lequel le gradient de la fonction à minimiser est nul) d'un problème d'optimisation sans contrainte. De tels points sont des minima globaux, si la fonction est convexe. Des extensions sont connues pour les problèmes avec contraintes simples, par exemple des contraintes de borne. Malgré des résultats de convergence théoriques satisfaisants, cet algorithme est généralement lent si le produit scalaire définissant le gradient ne varie pas avec le point courant de manière convenable, c'est-à-dire si l'espace vectoriel n'est pas muni d'une structure riemannienne appropriée, d'ailleurs difficilement spécifiable a priori. Il est donc franchement à déconseiller, même pour minimiser une fonction quadratique strictement convexe de deux variables[réf. nécessaire]. Toutefois, ses qualités théoriques font que l'algorithme sert de modèle à la famille des algorithmes à directions de descente ou de sauvegarde dans les algorithmes à régions de confiance.
Le principe de cet algorithme remonte au moins à Cauchy (1847).},
	language = {fr},
	urldate = {2023-11-09},
	journal = {Wikipédia},
	month = sep,
	year = {2023},
	note = {Page Version ID: 207455034},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\R4MASLG5\\Algorithme_du_gradient.html:text/html},
}

@misc{raschka_what_0000,
	title = {What are gradient descent and stochastic gradient descent?},
	url = {https://sebastianraschka.com/faq/docs/gradient-optimization.html},
	abstract = {Gradient Descent (GD) Optimization},
	language = {en},
	urldate = {2023-11-09},
	journal = {Sebastian Raschka, PhD},
	author = {Raschka, Sebastian},
	year = {0000},
}

@misc{noauthor_backpropagation_2023,
	title = {Backpropagation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Backpropagation&oldid=1184102952},
	abstract = {As a machine-learning algorithm, backpropagation performs a backward pass to adjust a neural network model's parameters, aiming to minimize error. In a multi-layered network, backpropagation uses the following steps:

Propagate training data through the model from input to predicted output by computing the successive hidden layers' outputs and finally the final layer's output (the feedforward step).
Adjust the model weights to reduce the error relative to the weights.
The error is typically the squared difference between prediction and target.
For each weight, the slope or derivative of the error is found, and the weight adjusted by a negative multiple of this derivative, so as to go downslope toward the minimum-error configuration.
This derivative is easy to calculate for final layer weights, and possible to calculate for one layer given the next layer's derivatives. Starting at the end, then, the derivatives are calculated layer by layer toward the beginning -- thus "backpropagation".
Repeatedly update the weights until they converge or the model has undergone enough iterations.It is an efficient application of the Leibniz chain rule (1673) to such networks. It is also known as the reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970).   The term "back-propagating error correction" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this, even though Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory.Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input–output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming. Gradient descent, or variants such as stochastic gradient descent, are commonly used.
Strictly the term backpropagation refers only to the algorithm for computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm – including how the gradient is used, such as by stochastic gradient descent. In 1986 David E. Rumelhart et al. published an experimental analysis of the technique. This contributed to the popularization of backpropagation and helped to initiate an active period of research in multilayer perceptrons.},
	language = {en},
	urldate = {2023-11-09},
	journal = {Wikipedia},
	month = nov,
	year = {2023},
	note = {Page Version ID: 1184102952},
}

@article{fleuret_deep_nodate-2,
	title = {Deep learning 3.6. {Back}-propagation},
	language = {en},
	journal = {Deep learning},
	author = {Fleuret, Francois},
	file = {Fleuret - Deep learning 3.6. Back-propagation.pdf:C\:\\Users\\David\\Zotero\\storage\\THQMNR6F\\Fleuret - Deep learning 3.6. Back-propagation.pdf:application/pdf},
}

@article{fleuret_deep_nodate-3,
	title = {Deep learning 3.5. {Gradient} descent},
	language = {en},
	journal = {Deep learning},
	author = {Fleuret, Francois},
	file = {Fleuret - Deep learning 3.5. Gradient descent.pdf:C\:\\Users\\David\\Zotero\\storage\\LDF423U5\\Fleuret - Deep learning 3.5. Gradient descent.pdf:application/pdf},
}

@article{fleuret_34_nodate,
	title = {3.4. {Multi}-{Layer} {Perceptrons}},
	language = {en},
	author = {Fleuret, Francois},
	file = {Fleuret - 3.4. Multi-Layer Perceptrons.pdf:C\:\\Users\\David\\Zotero\\storage\\9VNZSFAQ\\Fleuret - 3.4. Multi-Layer Perceptrons.pdf:application/pdf},
}

@article{fleuret_first_nodate,
	title = {The first mathematical model for a neuron was the {Threshold} {Logic} {Unit}, with {Boolean} inputs and outputs:},
	language = {en},
	author = {Fleuret, Francois},
	file = {Fleuret - The first mathematical model for a neuron was the .pdf:C\:\\Users\\David\\Zotero\\storage\\HE29YT3S\\Fleuret - The first mathematical model for a neuron was the .pdf:application/pdf},
}

@misc{noauthor_mean_2023,
	title = {Mean squared error},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Mean_squared_error&oldid=1181281408},
	abstract = {In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate. In machine learning, specifically empirical risk minimization, MSE may refer to the empirical risk (the average loss on an observed data set), as an estimate of the true MSE (the true risk: the average loss on the actual population distribution).
The MSE is a measure of the quality of an estimator.  As it is derived from the square of Euclidean distance, it is always a positive value that decreases as the error approaches zero.
The MSE is the second moment (about the origin) of the error, and thus incorporates both the variance of the estimator (how widely spread the estimates are from one data sample to another) and its bias (how far off the average estimated value is from the true value). For an unbiased estimator, the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated. In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error.},
	language = {en},
	urldate = {2023-11-08},
	journal = {Wikipedia},
	month = oct,
	year = {2023},
	note = {Page Version ID: 1181281408},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\RZ5A8CN9\\Mean_squared_error.html:text/html},
}

@phdthesis{schlag_jet_2018,
	title = {Jet {Reconstruction} in the {ATLAS} {Level}-1 {Calorimeter} {Trigger} with {Deep} {Artificial} {Neural} {Networks}},
	url = {https://cds.cern.ch/record/2670301},
	abstract = {In the upcoming years, the planned upgrades of the Large Hadron Collider (LHC) at the European Organization for Nuclear Research CERN will result in never before reached luminosities and very high pile-up conditions and will thus impose new chal- lenges on the experimental setups of the four big detectors located at the LHC. In order to maintain or even improve upon the current performance capabilities under these new challenging operating conditions, and hence guarantee a full exploitation of the rich physics potential provided by the LHC, also major upgrades of the ATLAS sub-detector systems are required. The ATLAS detector observes proton-proton collision events at the LHC bunch cross- ing rate of 40 MHz. In order to reduce the data flow to manageable levels and keep only the small fraction of events containing physics processes that are of interest for further analyses, efficient online data selection of interesting and rare processes out of the large data volume delivered by the LHC is essential for a collider experiment. The ATLAS trigger system is specifically designed for this important task. It consists of a hardware-based Level-1 trigger and a software-based High-Level trigger, reducing the event rate from the LHC bunch crossing rate to the detector read-out rate of about 1 kHz. Among others, the Level-1 trigger system involves the Level-1 Calorimeter Trigger (L1Calo) which is specialized on searches for e.g. high-energy electrons, photons, τ leptons and particle jets. Jets play crucial roles in many analyses like e.g. searches for new physics and are therefore important objects that need to be reconstructed in the trigger system. An efficient jet finding algorithm implemented on FPGAs in L1Calo is indispensable to cope with the upcoming very busy environments. By considering calorimeter information as images, state-of-the-art image recognition techniques like convolutional neural networks become promising candidates for this challenging task. In the last decade, the development of deep learning and image recognition techniques has made an enormous progress, often leading to novel algo- rithms that are able to outperform previous state-of-the-art techniques. Moreover, the Universal Approximation Theorem suggests that neural networks can be regarded as universal approximators for a variety of different problems. This motivates the consideration of utilising a deep artificial neural network for jet reconstruction in the ATLAS Level-1 Calorimeter Trigger. The development of a neural network that is capable of finding jets with an efficiency of the anti-kt jet clustering algorithm while simultaneously being small enough to al- low an implementation on the Level-1 trigger hardware is the goal of this thesis and will be presented in the following.},
	urldate = {2023-10-23},
	school = {Mainz U.},
	author = {Schlag, Bastian},
	year = {2018},
	annote = {Presented 20 Aug 2018},
	file = {Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\UCXYXRVI\\Schlag - 2018 - Jet Reconstruction in the ATLAS Level-1 Calorimete.pdf:application/pdf},
}

@article{achenbach_atlas_2008,
	title = {The {ATLAS} {Level}-1 {Calorimeter} {Trigger}},
	volume = {3},
	issn = {1748-0221},
	url = {https://iopscience.iop.org/article/10.1088/1748-0221/3/03/P03001},
	doi = {10.1088/1748-0221/3/03/P03001},
	abstract = {The ATLAS Level-1 Calorimeter Trigger uses reduced-granularity information from all the ATLAS calorimeters to search for high transverse-energy electrons, photons, τ leptons and jets, as well as high missing and total transverse energy. The calorimeter trigger electronics has a fixed latency of about 1 µs, using programmable custom-built digital electronics. This paper describes the Calorimeter Trigger hardware, as installed in the ATLAS electronics cavern.},
	language = {en},
	number = {03},
	urldate = {2023-10-23},
	journal = {Journal of Instrumentation},
	author = {Achenbach, R and Adragna, P and Andrei, V and Apostologlou, P and Åsman, B and Ay, C and Barnett, B M and Bauss, B and Bendel, M and Bohm, C and Booth, J R A and Brawn, I P and Thomas, P Bright and Charlton, D G and Collins, N J and Curtis, C J and Dahlhoff, A and Davis, A O and Eckweiler, S and Edwards, J P and Eisenhandler, E and Faulkner, P J W and Fleckner, J and Föhlisch, F and Garvey, J and Gee, C N P and Gillman, A R and Hanke, P and Hatley, R P and Hellman, S and Hidvégi, A and Hillier, S J and Jakobs, K and Johansen, M and Kluge, E -E and Landon, M and Lendermann, V and Lilley, J N and Mahboubi, K and Mahout, G and Mass, A and Meier, K and Moa, T and Moyse, E and Müller, F and Neusiedl, A and Nöding, C and Oltmann, B and Pentney, J M and Perera, V J O and Pfeiffer, U and Prieur, D P F and Qian, W and Rees, D L and Rieke, S and Rühr, F and Sankey, D P C and Schäfer, U and Schmitt, K and Schultz-Coulon, H -C and Schumacher, C and Silverstein, S and Staley, R J and Stamen, R and Stockton, M C and Tapprogge, S and Thomas, J P and Trefzger, T and Watkins, P M and Watson, A and Weber, P and Woehrling, E -E},
	month = mar,
	year = {2008},
	pages = {P03001--P03001},
	file = {Achenbach et al. - 2008 - The ATLAS Level-1 Calorimeter Trigger.pdf:C\:\\Users\\David\\Zotero\\storage\\6WF69H74\\Achenbach et al. - 2008 - The ATLAS Level-1 Calorimeter Trigger.pdf:application/pdf},
}

@misc{noauthor_about_nodate,
	title = {About},
	url = {https://atlas.cern/about},
	abstract = {Official public website for the ATLAS Experiment at CERN},
	language = {en},
	urldate = {2023-10-23},
	journal = {ATLAS Experiment at CERN},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\8HAMTN82\\about.html:text/html},
}

@misc{atlas_machine_2023,
	title = {Machine learning is revolutionising our understanding of particle “jets”},
	url = {https://atlas.cern/Updates/Briefing/Boosting-Jets-AI},
	abstract = {What happens when – instead of recording a single particle track or energy deposit in your detector – you see a complex collection of many particles, with many tracks, that leaves a large amount of energy in your calorimeters? Then congratulations: you’ve recorded a “jet”! Jets are the complicated experimental signatures left behind by showers of strongly-interacting quarks and gluons. By studying the internal energy flow of a jet – also known as the “jet substructure” – physicists can learn about the kind of particle that created it. For instance, several hypothesised new particles could decay into heavy Standard Model particles at extremely high (or “boosted”) energies. These particles could then decay into multiple quarks, leaving behind “boosted”, multi-pronged jets in the ATLAS experiment. Physicists use “taggers” to distinguish these jets from background jets created by single quarks and gluons. The type of quarks produced in the jet can also give extra information about the original particle. For example, Higgs bosons and top quarks often decay to b-quarks – seen in ATLAS as “b-jets” – which can be distinguished from other kinds of jets using the long lifetime of the B-hadron. The complexity of jets naturally lends itself to Artificial Intelligence (AI) algorithms, which are able to efficiently distil large amounts of information into accurate decisions. AI algorithms have been a regular part of ATLAS data analysis for several years, with ATLAS physicists continuously pushing these tools to new limits. This week, ATLAS physicists presented four exciting new results about jet tagging using AI algorithms at the BOOST 2023 conference held at Lawrence Berkeley National Lab (USA). Figure 1: The graphs showing the full declustering shower development and the primary Lund jet plane in red are shown in (left) for a jet originating from a W-boson and in (right) for a jet originating from a light-quark. (Image: ATLAS Collaboration/CERN) Artificial intelligence is revolutionising how ATLAS researchers identify – or "tag" – what types of particles create jets in the experiment. Two results showcased new ATLAS taggers used for identifying jets coming from a boosted W-boson decay as opposed to background jets originating from light quarks and gluons. Typically, AI algorithms are trained on “high-level” jet substructure information recorded by the ATLAS inner detector and calorimeters – such as the jet mass, energy correlation ratios and jet splitting scales. These new studies instead use “low-level” information from these same detectors – such as the direct kinematic properties of a jet’s constituents or the novel two-dimensional parameterisation of radiation within a jet (known as the “Lund Jet plane”), built from the jet’s constituents and using graphs based on the particle-shower development (see Figure 1). These new taggers made it possible to separate the shape of signal and background far more effectively than any high-level taggers could do alone (see Figure 2). In particular, the Lund Jet plane-based tagger outperforms the other methods, by using the same input to the AI networks but in a different format inspired by the physics of the jet shower development. A similar evolution was followed for the development of a new boosted Higgs tagger, which identifies jets originating from boosted Higgs bosons decaying hadronically to two b-quarks or c-quarks. It also uses low-level information – in this case, tracks reconstructed from the inner detector associated with the single jet containing the Higgs boson decays. This new tagger is the most performant tagger to date, and represents a factor of 1.6 to 2.5 improvement, at a 50\% boosted Higgs signal efficiency, over the previous version of the tagger, which used high-level information from the jet and b/c-quark decays as input for a neural network (see Figure 3). Figure 2: Signal efficiency as a function of the background rejection for the different W-boson taggers: one is based on the Lund jet plane, while the others use unordered sets of particles or graphs with additional structure. (Image: ATLAS Collaboration/CERN) Figure 3: Top and multijet rejections as a function of the H→bb signal efficiency. Performance of the new boosted Higgs tagger is compared to the previous taggers using high-level information from the jet b-quark decays. (Image: ATLAS Collaboration/CERN) Finally, ATLAS researchers presented two new taggers that aim to differentiate between jets originating from quarks and those originating from gluons. One tagger looked at the charged-particle constituent multiplicity of the jets being tagged, while the other combined several jet kinematic and jet substructure variables using a Boosted Decision Tree. Physicists compared the performance of these quark/gluon taggers; Figure 4 shows the rejection of gluon jets as a function of quark selection efficiency in simulation. Several studies of Standard-Model processes – including vector boson fusion – and new physics searches with quark-rich signals could greatly benefit from these taggers. However, in order for them to be used in analyses, additional corrections on the signal efficiency and background rejection need to be applied to bring the performance of the taggers in data and simulation to be the same. Researchers measured both the efficiency and rejection rates in Run-2 data for these taggers, and found good agreement between the measured data and predictions; therefore, only small corrections are needed. The excellent performance of these new jet taggers does not come without questions. Crucially, how can researchers interpret what the machine-learning models learned? And why do more complex architectures show a stronger dependence on the modelling of simulated physics processes used for the training, as shown in the two W-tagging studies? Challenges aside, these taggers set an outstanding baseline for analysing LHC Run-3 data. Given the current strides being made in machine learning, its continued application to particle physics will hopefully increase the understanding of jets and revolutionise the ATLAS physics programme in the years to come. Figure 4: Signal efficiency as a function of the background rejection for different quark taggers. The use of machine learning (BDT) results in an improved performance. (Image: ATLAS Collaboration/CERN) Learn more Tagging boosted W bosons with the Lund jet plane in ATLAS (ATL-PHYS-PUB-2023-017) Constituent-based W-boson tagging with the ATLAS detector (ATL-PHYS-PUB-2023-020) Transformer Neural Networks for Identifying Boosted Higgs Bosons decaying into bb and cc in ATLAS (ATL-PHYS-PUB-2023-021) Performance and calibration of quark/gluon-jet taggers using 140 fb−1 of proton–proton collisions at 13 TeV with the ATLAS detector (JETM-2020-02) Comparison of ML algorithms for boosted W boson tagging (JETM-2023-003) Summary of new ATLAS results from BOOST 2023, ATLAS News, 31 July 2023},
	language = {en},
	urldate = {2023-10-23},
	journal = {ATLAS},
	author = {ATLAS, Collaboration},
	month = aug,
	year = {2023},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\6VZG9L3Z\\Boosting-Jets-AI.html:text/html},
}

@misc{noauthor_trigger_nodate,
	title = {Trigger and {Data} {Acquisition} {System}},
	url = {https://atlas.cern/Discover/Detector/Trigger-DAQ},
	abstract = {Official public website for the ATLAS Experiment at CERN},
	language = {en},
	urldate = {2023-10-20},
	journal = {ATLAS Experiment at CERN},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\P4VMM3ZY\\Trigger-DAQ.html:text/html},
}

@misc{noauthor_large_2023,
	title = {The {Large} {Hadron} {Collider}},
	url = {https://home.web.cern.ch/science/accelerators/large-hadron-collider},
	language = {en},
	urldate = {2023-10-20},
	journal = {CERN},
	month = oct,
	year = {2023},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\8M52KSA8\\large-hadron-collider.html:text/html},
}

@misc{noauthor_qkeras_2023,
	title = {{QKeras}},
	copyright = {Apache-2.0},
	url = {https://github.com/google/qkeras},
	abstract = {QKeras: a quantization deep learning library for Tensorflow Keras},
	urldate = {2023-10-20},
	publisher = {Google},
	month = oct,
	year = {2023},
	note = {original-date: 2019-08-06T22:11:58Z},
	keywords = {accelerator, asic-design, deep-learning, fpga, fpga-accelerator, hardware-acceleration, keras, machine-learning, quantization, quantized-networks, quantized-neural-networks, tensorflow},
}

@misc{noauthor__nodate,
	title = {{\textbar} notebook.community},
	url = {https://notebook.community/google/qkeras/notebook/QKerasTutorial},
	urldate = {2023-10-20},
}

@misc{noauthor_qkerasnotebookqkerastutorialipynb_nodate,
	title = {qkeras/notebook/{QKerasTutorial}.ipynb at master · google/qkeras},
	url = {https://github.com/google/qkeras/blob/master/notebook/QKerasTutorial.ipynb},
	abstract = {QKeras: a quantization deep learning library for Tensorflow Keras - google/qkeras},
	language = {en},
	urldate = {2023-10-20},
	journal = {GitHub},
}

@misc{noauthor_hls4ml-tutorialpart6_cnnsipynb_nodate,
	title = {hls4ml-tutorial/part6\_cnns.ipynb at main · fastmachinelearning/hls4ml-tutorial},
	url = {https://github.com/fastmachinelearning/hls4ml-tutorial/blob/main/part6_cnns.ipynb},
	abstract = {Tutorial notebooks for hls4ml . Contribute to fastmachinelearning/hls4ml-tutorial development by creating an account on GitHub.},
	language = {en},
	urldate = {2023-10-20},
	journal = {GitHub},
}

@article{abhishek_resnet18_2022,
	title = {Resnet18 {Model} {With} {Sequential} {Layer} {For} {Computing} {Accuracy} {On} {Image} {Classification} {Dataset}},
	volume = {10},
	abstract = {This residual network has been a broad domain of research in deep learning. Many complex architectures are based upon residual networks. Residual networks are efficient due to skip connections. This paper highlights the addition of a sequential layer to the traditional RESNET 18 model for computing the accuracy of an Image classification dataset. The classification datasets such as Intel Scene dataset, CIFAR10 dataset, etc. These datasets consist of images belonging to various classes. In classification, we assign the pictures to their respective categories. The addition of a sequential layer gives the accuracy in the range of 0 to 1, which helps find the accuracy of prediction for the test set.},
	language = {en},
	number = {5},
	author = {Abhishek, Allena Venkata Sai and Gurrala, Dr Venkateswara Rao and Sahoo, Dr Laxman},
	year = {2022},
	file = {Abhishek et al. - 2022 - Resnet18 Model With Sequential Layer For Computing.pdf:C\:\\Users\\David\\Zotero\\storage\\J4ZQYJW6\\Abhishek et al. - 2022 - Resnet18 Model With Sequential Layer For Computing.pdf:application/pdf},
}

@misc{noauthor_jets_nodate,
	title = {Jets at {CMS} and the determination of their energy scale {\textbar} {CMS} {Experiment}},
	url = {https://cms.cern/news/jets-cms-and-determination-their-energy-scale},
	urldate = {2023-07-25},
	file = {Jets at CMS and the determination of their energy scale | CMS Experiment:C\:\\Users\\David\\Zotero\\storage\\T7JXVLEB\\jets-cms-and-determination-their-energy-scale.html:text/html},
}

@article{redmon_yolov3_2018,
	title = {{YOLOv3}: {An} {Incremental} {Improvement}},
	shorttitle = {{YOLOv3}},
	url = {http://arxiv.org/abs/1804.02767},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = apr,
	year = {2018},
	note = {arXiv:1804.02767 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech Report},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\3J2RVCBN\\1804.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\69V9QF8W\\Redmon and Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf:application/pdf},
}

@inproceedings{shelton_jet_2013,
	address = {Boulder, Colorado},
	title = {Jet {Substructure}},
	isbn = {978-981-4525-21-3 978-981-4525-22-0},
	url = {http://www.worldscientific.com/doi/abs/10.1142/9789814525220_0007},
	doi = {10.1142/9789814525220_0007},
	abstract = {Jet physics is a rich and rapidly evolving field, with many applications to
physics in and beyond the Standard Model. These notes, based on lectures delivered at the June 2012 Theoretical Advanced Study Institute,
provide an introduction to jets at the Large Hadron Collider. Topics
covered include sequential jet algorithms, jet shapes, jet grooming, and
boosted Higgs and top tagging.},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Searching for {New} {Physics} at {Small} and {Large} {Scales}},
	publisher = {WORLD SCIENTIFIC},
	author = {Shelton, J.},
	month = nov,
	year = {2013},
	pages = {303--340},
}

@article{amerl_atlas_2023,
	title = {The {ATLAS} jet trigger for {Run} 3 of the {LHC}},
	url = {https://cds.cern.ch/record/2868263},
	abstract = {The ATLAS jet trigger is instrumental in selecting events both for Standard Model measurements and Beyond the Standard Model physics searches. Non-standard triggering strategies, such as saving only a small fraction of trigger objects for each event, avoids bandwidth limitations and increases sensitivity to low-mass and low-momentum objects. These events are used by Trigger Level Analyses, which can reach regions of parameter space that would otherwise be inaccessible. To this end, the calibration of trigger-level jets is imperative both to ensure good trigger performance across the ATLAS physics programme and to provide well-measured jets for Trigger Level Analysis. This contribution presents an introduction to the ATLAS jet trigger for Run-3 of the LHC and discusses the performance of the trigger jet calibration. These studies will allow us to commission a Run-3 trigger jet calibration that provides excellent performance across a broad jet transverse momentum range as low as 25 GeV.},
	urldate = {2024-01-15},
	publisher = {CERN},
	author = {Amerl, Maximilian},
	year = {2023},
	note = {Place: Geneva},
	file = {Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\Q62LCQ32\\Amerl - 2023 - The ATLAS jet trigger for Run 3 of the LHC.pdf:application/pdf},
}

@article{cogan_jet-images_2015,
	title = {Jet-images: computer vision inspired techniques for jet tagging},
	volume = {2015},
	issn = {1029-8479},
	shorttitle = {Jet-images},
	url = {https://doi.org/10.1007/JHEP02(2015)118},
	doi = {10.1007/JHEP02(2015)118},
	abstract = {We introduce a novel approach to jet tagging and classification through the use of techniques inspired by computer vision. Drawing parallels to the problem of facial recognition in images, we define a jet-image using calorimeter towers as the elements of the image and establish jet-image preprocessing methods. For the jet-image processing step, we develop a discriminant for classifying the jet-images derived using Fisher discriminant analysis. The effectiveness of the technique is shown within the context of identifying boosted hadronic W boson decays with respect to a background of quark- and gluoninitiated jets. Using Monte Carlo simulation, we demonstrate that the performance of this technique introduces additional discriminating power over other substructure approaches, and gives significant insight into the internal structure of jets.},
	language = {en},
	number = {2},
	urldate = {2024-01-15},
	journal = {Journal of High Energy Physics},
	author = {Cogan, Josh and Kagan, Michael and Strauss, Emanuel and Schwarztman, Ariel},
	month = feb,
	year = {2015},
	keywords = {Hadronic Colliders, Jets},
	pages = {118},
	file = {Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\XICYP45B\\Cogan et al. - 2015 - Jet-images computer vision inspired techniques fo.pdf:application/pdf},
}

@article{gholami_survey_2021,
	title = {A {Survey} of {Quantization} {Methods} for {Efficient} {Neural} {Network} {Inference}},
	url = {http://arxiv.org/abs/2103.13630},
	abstract = {As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt},
	month = jun,
	year = {2021},
	note = {arXiv:2103.13630 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Book Chapter: Low-Power Computer Vision: Improving the Efficiency of Artificial Intelligence},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\8JX779H6\\2103.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\I3PKE2QN\\Gholami et al. - 2021 - A Survey of Quantization Methods for Efficient Neu.pdf:application/pdf},
}

@article{coelho_jr_ultra_2020,
	title = {Ultra {Low}-latency, {Low}-area {Inference} {Accelerators} using {Heterogeneous} {Deep} {Quantization} with {QKeras} and hls4ml},
	url = {https://arxiv.org/pdf/2006.10159v1.pdf},
	abstract = {In this paper, we introduce the QKeras library, an extension of the
Keras library allowing for the creation of heterogeneously quantized versions of deep neural network models, through drop-in
replacement of Keras layers. These models are trained quantizationaware, where the user can trade off model area or energy consumption by accuracy. We demonstrate how the reduction of numerical
precision, through quantization-aware training, significantly reduces resource consumption while retaining high accuracy when
implemented on FPGA hardware. Together with the hls4ml library,
this allows for a fully automated deployment of quantized Keras
models on chip, crucial for ultra low-latency inference. As a benchmark problem, we consider a classification task for the triggering
of events in proton-proton collisions at the CERN Large Hadron
Collider, where a latency of O(1) µs is required.},
	urldate = {2024-01-15},
	author = {Coelho Jr., Claudionor N. and Kuusela, Aki and Zhuang, Hao and Aarrestad, Thea and Loncar, Vladimir and Ngadiuba, Jennifer and Pierini, Maurizio and Summers, Sioni},
	month = jun,
	year = {2020},
	note = {arXiv:2006.10159 [hep-ex, physics:physics]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Electrical Engineering and Systems Science - Signal Processing, High Energy Physics - Experiment, Physics - Instrumentation and Detectors},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\F8BJUDZI\\2006.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\RBL2DVSL\\Coelho Jr. et al. - 2021 - Automatic heterogeneous quantization of deep neura.pdf:application/pdf},
}

@article{aad_atlas_2023,
	address = {Geneva},
	title = {The {ATLAS} {Experiment} at the {CERN} {Large} {Hadron} {Collider}: {A} {Description} of the {Detector} {Configuration} for {Run} 3},
	shorttitle = {The {ATLAS} {Experiment} at the {CERN} {Large} {Hadron} {Collider}},
	url = {https://cds.cern.ch/record/2859916},
	abstract = {The ATLAS detector is installed in its experimental cavern at Point 1 of the CERN Large Hadron Collider. During Run 2 of the LHC, a luminosity of \${\textbackslash}mathcal\{L\}=2{\textbackslash}times 10{\textasciicircum}\{34\}{\textbackslash}mathrm\{cm\}{\textasciicircum}\{-2\}{\textbackslash}mathrm\{s\}{\textasciicircum}\{-1\}\$ was routinely achieved at the start of fills, twice the design luminosity. For Run 3, accelerator improvements, notably luminosity levelling, allow sustained running at an instantaneous luminosity of \${\textbackslash}mathcal\{L\}=2{\textbackslash}times 10{\textasciicircum}\{34\}{\textbackslash}mathrm\{cm\}{\textasciicircum}\{-2\}{\textbackslash}mathrm\{s\}{\textasciicircum}\{-1\}\$, with an average of up to 60 interactions per bunch crossing. The ATLAS detector has been upgraded to recover Run 1 single-lepton trigger thresholds while operating comfortably under Run 3 sustained pileup conditions. A fourth pixel layer 3.3 cm from the beam axis was added before Run 2 to improve vertex reconstruction and \$b\$-tagging performance. New Liquid Argon Calorimeter digital trigger electronics, with corresponding upgrades to the Trigger and Data Acquisition system, take advantage of a factor of 10 finer granularity to improve triggering on electrons, photons, taus, and hadronic signatures through increased pileup rejection. The inner muon endcap wheels  were replaced by New Small Wheels with Micromegas and small-strip Thin Gap Chamber detectors, providing both precision tracking and Level-1 Muon trigger functionality. Tile Calorimeter scintillation counters were added to improve electron energy resolution and background rejection. Upgrades to Minimum Bias Trigger Scintillators and Forward Detectors improve luminosity monitoring and enable total proton-proton cross section, diffractive physics, and heavy ion measurements. These upgrades are all compatible with operation in the much harsher environment anticipated after the High-Luminosity upgrade of the LHC and are the first steps towards preparing ATLAS for the High-Luminosity upgrade of the LHC. This paper describes the Run 3 configuration of the ATLAS detector.},
	urldate = {2024-01-15},
	publisher = {CERN},
	author = {Aad, Georges and Abbott, Braden Keim and Abbott, Dale and Abdallah, Jalal and Abeling, Kira and Abidi, Haider and Aboulhorma, Asmaa and Abovyan, Sergey and Abramowicz, Halina and Abreu, Henso and Abulaiti, Yiming and Abusleme, Angel and Acharya, Bobby Samir and Adam Bourdarios, Claire and Adamczyk, Leszek and Adamek, Lukas and Addepalli, Sagar and Adelman, Jahred and Adersberger, Michael and Adiguzel, Aytul and Adorni Braccesi Chiassi, Sofia and Adye, Tim and Affolder, Tony and Afik, Yoav and Agaras, Merve Nazlim and Agarwala, Jinky and Aggarwal, Anamika and Agheorghiesei, Catalin and Aguilar Saavedra, Juan Antonio and Ahmad, Ammara and Ahmadov, Faig and Ahmed, Waleed Syed and Ahuja, Sudha and Ai, Xiaocong and Aielli, Giulio and Ait Tamlihat, Malak and Aitbenchikh, Brahim and Aizenberg, Iakov and Akbiyik, Melike and Akesson, Torsten and Akhperjanyan, Gevorg and Akimov, Andrei and Al Khoury, Konie and Alberghi, Gian Luigi and Albert, Justin and Albicocco, Pietro and Alderweireldt, Sara and Aleksa, Martin and Alexandrov, Igor and Alexa, Calin and Alexopoulos, Theodoros and Alfonsi, Alice and Alfonsi, Fabrizio and Alhroob, Muhammad and Ali, Babar and Ali, Shahzad and Aliev, Malik and Alimonti, Gianluca and Alkakhi, Wael and Allaire, Corentin and Allard, Jérôme and Allbrooke, Benedict and Allendes Flores, Cristian Andres and Allport, Philip Patrick and Aloisio, Alberto and Alonso, Francisco and Alpigiani, Cristiano and Alvarez Estevez, Manuel and Alvarez Gonzalez, Barbara and Alviggi, Mariagrazia and Aly, Mohamed and Do Amaral Coutinho, Yara and Ambler, Alessandro and Amelung, Christoph and Amerl, Maximilian and Ames, Christoph and Amidei, Dante and Amor Dos Santos, Susana Patricia and Amos, Kieran Robert and Ananiev, Viktor and Anastopoulos, Christos and Andari, Nansi and Andeen, Timothy Robert and Anders, John Kenneth and Andrean, Stefio Yosse and Andreazza, Attilio and Anelli, Christopher Ryan and Angelidakis, Stylianos and Angerami, Aaron and Anisenkov, Alexey and Annovi, Alberto and Antel, Claire and Anthony, Matthew Thomas and Antipov, Egor and Antonelli, Mario and Antonescu, Mihai and Antrim, Daniel Joseph and Anulli, Fabio and Aoki, Masato and Aoki, Takumi and Aparisi Pozo, Javier Alberto and Aparo, Marco and Aperio Bella, Ludovica and Appelt, Christian and Aranzabal Barrio, Nordin and Araujo Ferraz, Victor and Arcangeletti, Chiara and Arce, Ayana Tamu and Arena, Eloisa and Arguin, Jean-Francois and Argyris, Anastasios and Argyropoulos, Spyros and Arling, Jan-Hendrik and Armbruster, Aaron James and Armijo, Charles Edward and Arnaez, Olivier and Arnold, Hannah and Arrubarrena Tame, Zulit Paola and Artoni, Giacomo and Asada, Haruka and Asai, Kanae and Asai, Shoji and Asbah, Nedaa Alexandra and Assahsah, Jihad and Assamagan, Ketevi Adikle and Astalos, Robert and Atkin, Ryan Justin and Atkinson, Markus Julian and Atlay, Naim Bora and Atmani, Hicham and Atmasiddha, Prachi and Aubernon, Erwann and Augsten, Kamil and Aune, Aune and Auricchio, Silvia and Auriol, Adrien and Aust, Florian and Austrup, Volker Andreas and Avner, Gal and Avolio, Giuseppe and Avoni, Giulio and Axen, David and Axiotis, Konstantinos and Aydiner, Pelin and Ayoub, Mohamad Kassem and Azaryan, Tatiana and Azuelos, Georges and Babal, Dominik and Bachacou, Henri and Bachas, Konstantinos and Bachiu, Alexander and Backman, Karl Filip and Badea, Anthony and Bagnaia, Paolo and Bahmani, Marzieh and Bailey, Adam and Bailey, Virginia and Baines, John and Bakalis, Christos and Baker, Keith and Bakker, Pepijn Johannes and Bakos, Evelin and Bakshi Gupta, Debottam and Balaji, Shyam and Balasubramanian, Rahul and Balbi, Gabriele and Baldin, Evgenii and Balek, Petr and Ball, Robert and Ballabene, Eric and Ballansat, Jacques and Balli, Fabrice and Baltes, Lisa Marie and Balunas, William Keaton and Balz, Johannes and Ban, Jaroslav and Banas, Elzbieta and Bandieramonte, Marilena and Bandyopadhyay, Anjishnu and Bansal, Shubham and Barak, Liron and Barberio, Elisabetta and Barberis, Dario and Barbero, Marlon Benoit and Barbier, Gerard and Barbour, Gregory and Bardo, Laetitia and Barends, Kevin Nicholas and Barfusser, Anja and Barillari, Teresa and Barisits, Martin and Barklow, Tim and Barnett, Michael and Baron, Petr and Baron, Diego and Baroncelli, Toni and Barone, Gaetano and Barr, Alan and Barranco Navarro, Laura and Barreiro Alonso, Fernando and Barreiro Guimaraes Da Costa, Joao and Barron, Uriel and Barros, Maura and Barsov, Sergey and Bartels, Falk and Bartoldus, Rainer and Barton, Adam Edward and Bartos, Pavol and Basalaev, Artem and Basan, Alexander and Baselga Bacardit, Marta and Bashta, Inna and Bassalat, Ahmed and Basso, Matthew Joseph and Basson, Candice Ruth and Bates, Richard and Batlamous, Souad and Batley, Richard and Batool, Binish and Battaglia, Marco and Battulga, Daariimaa and Bauce, Matteo and Bauer, Patrick and Bayirli, Arif and Beacham, James and Beau, Tristan and Belhesan, Fredy and Beltramelli, Beltramelli and Beauchamp, Blake Christopher and Beauchemin, Pierre-Hugues and Beccherle, Roberto and Becherer, Fabian and Bechtle, Philip and Beck, Hans Peter and Becker, Kathrin and Beddall, Andrew and Bednyakov, Vadim and Bee, Chris and Beemster, Lars and Beermann, Thomas and Begalli, Marcia and Begel, Michael and Behera, Arabinda and Behr, Janna Katharina and Beirao Da Cruz E Silva, Cristovao and Beirer, Joshua Falco and Beisiegel, Florian and Belanger-Champagne, Camille and Belfkir, Mohamed and Bella, Gideon and Bellachia, Fatih and Bellagamba, Lorenzo and Bellerive, Alain and Bellos, Panagiotis and Beloborodov, Konstantin and Belotskiy, Konstantin and Belyaev, Nikita and Raviv Moshe, Meny and Benchekroun, Driss and Bendebba, Fatima and Bendotti, Jerome and Benhammou, Yan and Benjamin, Doug and Benoit, Mathieu and Benoit, Theophile Arthur and Bensinger, Jim and Bentvelsen, Stan and Beresford, Lydia Audrey and Beretta, Matteo Mario and Bergeaas Kuutmann, Elin and Berger, Nicolas and Bergmann, Benedikt Ludwig and Beringer, Juerg and Berlendis, Simon and Bernardi, Gregorio and Bernius, Catrin and Bernlochner, Florian Urs and Bernon, Florent and Berry, Tracey and Berta, Peter and Berthold, Anne-Sophie and Bertram, Iain and Bervas, Hervé and Besin, Dominique and Bessudo, Ilan and Bethke, Siegfried and Betti, Alessandra and Bevan, Adrian and Bey, Bey and Bhamjee, Muaaz and Bhatta, Somadutta and Bhattacharya, Deb Sankar and Bhattarai, Prajita and Bhopatkar, Vallary Shashikant and Bi, Ran and Bianchi, Riccardo Maria and Bianga, Yves and Biaut, Mathieu and Biebel, Otmar and Bielski, Rafal and Biglietti, Michela and Billoud, Thomas and Bindi, Marcello and Bingul, Ahmet and Bini, Cesare and Biondini, Alessandro and Bira, Calin and Birch-Sykes, Callum Jacob and Bird, Gareth Adam and Birman, Mattias and Birney, Paul and Biros, Marek and Bisanz, Tobias and Bisceglie, Emanuele and Biswas, Diptaparna and Bita, Daniel and Bitadze, Alexander and Bjoerke, Kristian and Blaszczyk, Tomasz Piotr and Bloch, Ingo and Blocker, Craig and Blue, Andrew James and Blumenschein, Ulla and Blumenthal, Julian and Bobbink, Gerjan and Bobrovnikov, Viktor and Boehler, Michael and Bohm, Burkhard and Bogavac, Danijela and Bogdanchikov, Alexander and Bohm, Christian and Boisvert, Veronique and Bokan, Petar and Bold, Tomasz and Boline, Daniel Dooley and Bomben, Marco and Bona, Marcella and Bonini, Filiberto and Boonekamp, Maarten and Booth, Callum Dale and Borbely, Albert Gyorgy and Borecka-Bielska, Hanna Maria and Borgna, Lucas Santiago and Borissov, Guennadi and Bortfeldt, Jona and Bortoletto, Daniela and Bortolin, Claudio and Boscherini, Davide and Fernandez-Bosman, Martine and Bossio, Jonathan and Botte, James and Bouaouda, Khalil and Bouaziz, Saïd and Bouchhar, Naseem and Boudreau, Joseph and Bouedo, Thierry and Bouhova-Thacker, Eva and Boumediene, Djamel Eddine and Bouquet, Romain and Boveia, Antonio and Boyd, Jamie and Boye, Diallo and Boyko, Igor and Braam, Nick and Bracinik, Juraj and Braga Lisboa, Pedro Henrique and Brahimi, Nihal and Brandt, Gerhard Immanuel and Brandt, Oleg and Braren, Frued Erik and Brau, Benjamin Paul and Brau, Jim and Brawn, Ian and Brendlinger, Kurt and Schimmel Brener, Roy and Brenner, Lydia and Brenner, Richard and Bressler, Shikma and Breugnon, Patrick and Britton, David and Britzger, Daniel Andreas and Brock, Ian and Brooijmans, Gustaaf and Brooks, William King and Brost, Elizabeth and Brown, Leesa Marea and Bruce, Laura Elaine and Bruckler, Tim Lukas and Bruckman De Renstrom, Pawel and Bruers, Ben and Bruncko, Dusan and Bruni, Alessia and Bruni, Graziano and Brunner, Kathrin Michaela and Bruschi, Marco and Bruscino, Nello and Buanes, Trygve and Buat, Quentin and Buchholz, Peter and Buckley, Andy and Buda, Stelian and Budagov, Ioulian and Bugge, Magnar Kopangen and Bulekov, Oleg and Bullard, Brendon and Burdin, Sergey and Burgard, Carsten and Burger, Angela Maria and Burghgrave, Blake Oliver and Burr, Jon and Burton, Charles and Burzynski, Jackson Carl and Busch, Elena Laura and Buescher, Volker and Bussey, Peter John and Butler, John Mark and Buttar, Craig Macleod and Butterworth, Jonathan and Buttinger, Will and Buxo Vazquez, Carlos Josue and Buzykaev, Alexey and Cabras, Grazia and Cabrera Urban, Susana and Cadoux, Frank Raphael and Caforio, Davide and Cai, Huacheng and Cai, Yuchen and Cairo, Valentina and Cakir, Orhan and Calabro, Domenico and Calace, Noemi and Calafiura, Paolo and Calderini, Giovanni and Calfayan, Philippe and Callea, Giuseppe and Caloba, Luiz and Calvet, David and Calvet, Samuel and Calvet, Thomas Philippe and Calvetti, Milene and Camacho Toro, Reina Coromoto and Camarda, Stefano and Camarero Munoz, Daniel and Camarri, Paolo and Camerlingo, Maria Teresa and Cameron, David and Camincher, Clement and Campanelli, Mario and Camplani, Alessandra and Canale, Vincenzo and Canesse, Auriane and Bret Cano, Marc and Cantero Garcia, Josu and Cao, Nina Yuan Yuan and Cao, Yumeng and Cap, Sebastien and Capitolo, Emilio and Capocasa, Francesca and Capradossi, Giulio and Capua, Marcella and Cara, Gael Benjamin and Carbone, Antonio and Cardarelli, Roberto and Cardenas, Juan Carlos, Jr. and Cardillo, Fabio and Cardot, Charles Andre and Carli, Tancredi and Carlino, Giampaolo and Carlotta, Mathilde Doriane and Carlotto, Juan Ignacio and Carlson, Ben and Carlson, Evan Michael and Carlson, Katelyn Joyce and Carminati, Leonardo and Carnesale, Maria and Caron, Sascha and Carquin Lopez, Edson and Carra, Sonia and Carratta, Giuseppe and Carrio Argos, Fernando and Carter, Joseph and Carter, Thomas Michael and Casado Lechuga, Pilar and Caserio, Alessandro and Casha, Albert Francis and Cassese, Ciro and Castiglia, Emma Grace and Castillo, Florencia Luciana and Castillo Garcia, Lucia and Castillo Gimenez, Victoria and Castro, Nuno and Catinaccio, Andrea and Catmore, James and Cavaliere, Viviana and Cavalli, Noemi and Cavasinni, Vincenzo and Celebi, Emre and Celli, Federico and Centonze, Martino Salomone and Ceradini, Filippo and Cerny, Karel and Santiago Cerqueira, Augusto and Cerri, Alex and Cerrito, Lucio and Cerutti, Fabio and Cervelli, Alberto and Cetin, Serkant and Chadi, Zakaria and Chakraborty, Dhiman and Chala, Mikael and Chaleil, Thierry and Chan, Jay and Chan, Stephen Kam-Wah and Chan, Wai Yuen and Chapman, John Derek and Chapman, Jay and Chargeishvili, Bakar and Charlton, Dave and Charman, Thomas Paul and Chatterjee, Meghranjana and Chau, Chav Chhiv and Chekanov, Sergei and Chekulaev, Sergey and Shelkov, G. and Chen, Andy and Chen, Boping and Chen, Charlie and Chen, Huirun and Chen, Hucheng and Chen, Jing and Chen, Jiayi and Chen, Kai and Chen, Olay and Chen, Shion and Chen, Shenjian and Chen, Xiang and Chen, Xin and Chen, Ye and Cheng, Alkaid and Cheng, Hok Chuen Tom and Cheong, Sanha and Cheplakov, Alexander and Cheremushkina, Evgeniya and Cherepanova, Elizaveta and Cherkaoui El Moursli, Rajaa and Cheu, Elliott and Cheung, Kingman and Chevalier, Laurent and Chevillot, Nicolas and Chiarella, Vitaliano and Chiarelli, Giorgio and Chiedde, Nemer and Chiodini, Gabriele and Chisholm, Andrew Stephen and Chitan, Adrian and Chitishvili, Mariam and Chiu, Justin and Chizhov, Mihail and Choi, Kyungeon and Chomont, Arthur and Chou, Yuan-Tang and Chow, Edwin and Chowdhury, Tasnuva and Christopher, Lawrence Davou and Chrul, Anna and Chu, Michael Kwok Lam and Chu, Ming Chung and Chu, Xiaotong and Chudoba, Jiri and Chwastowski, Janusz and Ciapetti, Guido and Ciapetti, Marco and Constable, Miles and Corbaz, Florian and Ciecko, Robert Piotr and Cieri, Davide and Ciesla, Krzysztof and Cindro, Vladimir and Ciocio, Alessandra and Cirotto, Francesco and Citron, Zvi and Citterio, Mauro and Ciubotaru, Dan Andrei and Ciungu, Bianca Monica and Clark, Allan and Clark, Brian Lee and Clark, Philip and Clavijo Columbie, Jose Manuel and Clawson, Savannah and Cleland, Bill and Clemens, Jcc and Clement, Christophe and Clercx, Joshua and Clissa, Luca and Coadou, Yann and Cobal, Marina and Coccaro, Andrea and Barrue, Ricardo and Coelho Lopes De Sa, Rafael and Coelli, Simone and Cohen, Gil and Cohen, Hadar and Coimbra, Artur and Cole, Brian and Coliban, Radu Mihai and Collot, Johann and Conde Muino, Patricia and Connell, Matt and Connell, Simon and Connelly, Ian Allan and Conroy, Eimear Isobel and Conventi, Francesco and Cooke, Harry and Sarkar, Amanda and Cormier, Felix and Corpe, Louie Dartmoor and Corradi, Massimo and Corrigan, Eric Edward and Corriveau, Francois and Corsetti, Sabrina and Cortes Gonzalez, Arely and Costa Mezquita, Maria Jose and Costa De Paiva, Thiago and Costanza, Francesco and Costanzo, Davide and Cote, Benjamin and Cowan, Glen and Cowley, James William and Cranmer, Kyle Stuart and Crepe-Renaudin, Sabine and Crescioli, Francesco and Crespo-Lopez, Olivier and Cristinziani, Markus and Cristoforetti, Marco and Croft, Vincent Alexander and Crosetti, Nanni and Cueto Gomez, Ana Rosario and Cuhadar Donszelmann, Tulay and Cui, Han and Cui, Zhaoyuan and Cunningham, Liam and Curcio, Francesco and Czodrowski, Patrick Karl and Czurylo, Marta and Sousa, Mario Jose and Da Fonseca Pinto, Joao Victor and Da Via, Cinzia and Dabrowski, Wladyslaw and Dado, Tomas and Daguin, Jerome and Dahbi, Salah-Eddine and Dai, Tiesheng and Dallapiccola, Carlo and Dam, Mogens and D'Amen, Gabriele and D'Amico, Valerio and Damp, Johannes Frederic and Dandoy, Jeff and Daneri, Maria Florencia and Danielsson, Hans and Danielyan, Varuzhan and Danilevich, Evgueni and Danninger, Matthias and Dao, Valerio and Darbo, Nanni and Darmora, Smita and Das, Sruthy Jyothi and D'Auria, Saverio and David, Claire and David, Pierre-Yves and Davidek, Tomas and Davis, Douglas Raymond and Davis, Paul Matthew and Davis-Purcell, Benjamin Richard and Davoine, Loic and Dawson, Ian and De, Kaushik and De Asmundis, Riccardo and De Beurs, Marcus and De Biase, Nicola and De Castro, Stefano and De Cecco, Sandro and De Fazio, Benedetto and De Geronimo, Gianluigi and De Groot, Nicolo and De Jong, Paul and De Jong, Samuel Rudy and De La Torre Perez, Hector and De Maria, Antonio and De Salvo, Alessandro and De Sanctis, Umberto and De Santo, Antonella and De Vivie De Regie, Jean-Baptiste and De Olivaira, Rui and Decock, Gilles and Dedovich, Dmitri and Degens, Jordy and Degrange, Jordan and Deiana, Allison Mccarn and Del Corso, Francesca and Del Peso, Jose and Del Rio, Fer and Delebecque, Pierre and Deliot, Frederic and Delitzsch, Chris Malena and Della Pietra, Massimo and Della Volpe, Domenico and Dell'Acqua, Andrea and Dell'Asta, Lidia and Delmastro, Marco and Delsart, Pierre Antoine and Demers Konezny, Sarah Marie and Demichev, Mikhail and Deng, Binwei and Denisov, Serguei and D'Eramo, Louis and Derendarz, Dominik Karol and Derue, Frederic and Dervan, Paul and Desch, Klaus and Deschamps, Herve and Desforge, Daniel and Dette, Karola and Deutsch, Christopher and Di Bello, Francesco Armando and Di Ciaccio, Anna and Di Ciaccio, Lucia and Di Domenico, Antonio and Di Donato, Camilla and Di Girolamo, Alessandro and Di Gregorio, Giulia and Di Luca, Andrea and Di Micco, Biagio and Di Nardo, Roberto and Di Petrillo, Karri Folan and Di Stante, Luigi and Diaconu, Cristinel and De Almeida Dias, Flavia and Vale, Tiago and Diaz Gutierrez, Marco Aurelio and Diaz Capriles, Federico Guillermo and Didenko, Mariia and Diehl, Edward and Diehl, Leena and Dietsche, Wolfgang Albert and Diez Cornell, Sergio and Diez Pardos, Carmen and Dik, Alexey and Dikic, Nikola and Dima, Katerina and Dimitriadi, Christina and Dimitrievska, Aleksandra and Ding, Wenxiang and Dingfelder, Jochen Christian and Dinkespiler, Bernard and Dinu, Ioan-Mihail and Disset, Gaël and Dittmeier, Sebastian and Dittus, Fido and Djama, Fares and Djobava, Tamar and Djuvsland, Julia Isabell and Dobos, Daniel and Doglioni, Caterina and Dolejsi, Jiri and Dolezal, Zdenek and Donadelli, Marisilvia and Dondero, Paolo and Dong, Binbin and Donini, Julien Noce and D'Onofrio, Adelina and D'Onofrio, Monica and Dopke, Jens and Dorholt, Ole and Doria, Alessandra and Doubek, Martin and Dova, Maria Teresa and Doyle, Tony and Draguet, Maxence and Drechsler, Eric and Dreyer, Etienne and Drivas-Koulouris, Ioannis and Drobac, Alec Swenson and Drozdova, Mariia and Du, Dongshuo and Du, Yanyan and Du Pree, Tristan Arnoldus and Duan, Yanyun and Dubinin, Filipp and Dubovsky, Michal and Duchovni, Ehud and Duckeck, Guenter and Ducu, Otilia Anamaria and Duda, Dominik and Dudarev, Alexey and Dudder, Andreas Christian and D'Uffizi, Daniele and D'Uffizi, Matteo and Duflot, Laurent and Duehrssen-Debling, Michael and Duim, Rowan M. and Dulsen, Carsten and Dumitriu, Ana Elena and Dumont Dayot, Nicolas and Dunford, Monica and Dunford, Matthew Gordon and Dungs, Sascha and Dunne, Katherine Elaine and Duperrin, Arnaud and Yildiz, Hatice and Dueren, Michael Johannes and Durglishvili, Archil and Dury, Bryan and Dushkin, Andrey and Dwyer, Brianna and Dyckes, Ian and Dyndal, Mateusz and Dysch, Samuel Dezso and Dziedzic, Bartosz Sebastian and Dziurdzia, Piotr and Earnshaw, Zoe Olivia and Eckerova, Barbora and Eggebrecht, Stephen and Eggleston, Michael Glenn and Purcino De Souza, Edmar Egidio and Ehrke, Lukas and Eigen, Gerald and Einsweiler, Kevin Frank and Ekelof, Tord Johan Carl and Ekman, Per Alexander and El Ghazali, Yassine and El Jarrari, Hassnae and El Moussaouy, Ali and Ellajosyula, Venugopal and Ellert, Mattias and Elles, Sabine and Ellinghaus, Frank and Elliot, Alison and Ellis, Nick and Elmsheuser, Johannes and Elsing, Markus and Emeliyanov, Dmitry and Emerman, Alex and Enari, Yuji and Ene, Irina and Epari, Shalini and Erdmann, Johannes and Ereditato, Antonio and Erland, Paula Agnieszka and Errenst, Martin and Escalier, Marc and Escobar Ibanez, Carlos and Etzion, Erez and Gaspar De Andrade Evans, Guiomar and Evans, Hal and Evans, Meirin Oan and Eyring, Andreas and Ezhilov, Aleksei and Ezzarqtouni, Sanae and Fabbri, Federica and Fabbri, Laura and Facini, Gabriel and Fadeyev, Vitaliy and Fakhrutdinov, Rinat and Falchieri, Davide and Falciano, Speranza and Falda Coelho, Luis and Falke, Peter Johannes and Falke, Saskia and Falou, Aboud and Falsetti, Gregorio and Faltova, Jana and Fan, Yunyun and Fang, Yaquan and Fanourakis, Georgios and Fanti, Marcello and Faraj, Mohammed and Farazpay, Zahra and Farbin, Amir and Farilla, Ada and Farina, Edoardo Maria and Farooque, Trisha and Farrell, Jason and Farrington, Sinead and Fassi, Farida and Fassouliotis, Dimitris and Faszer, Wayne and Faucci Giannelli, Michele and Fausten, Camille and Favareto, Andrea and Fawcett, William James and Fayard, Louis and Febvre, Damien Romaric and Federicova, Pavla and Fedin, Oleg and Fedotov, Gleb and Feickert, Matthew and Feligioni, Lorenzo and Fell, Alix and Fellers, Deion Elgin and Felt, Nathan and Feng, Cunfeng and Feng, Minyu and Feng, Zhuoran and Fenton, Michael James and Fenyuk, Alexandre and Ferencz, Lars and Ferguson, Ruby Alice Molly and Fernandez Luengo, Sergio Ivan and Ferrando, James and Ferrari, Arnaud and Ferrari, Pamela and Ferrari, Roberto and Dias, Ualison and Ferrer Ribas, Esther and Ferrere, Didier and Ferretti, Claudio and Fiedler, Frank and Fielitz, William and Filippov, Yury and Filipcic, Andrej and Filmer, Emily and Filthaut, Frank and Castro Nunes Fiolhais, Miguel and Fiore, Giuseppe and Fiorini, Luca and Fischer, Alexander Peter and Leitgeb, Florian Christoph and Fisher, Wade Cameron and Fitschen, Tobias and Fleck, Ivor and Fleischmann, Philipp and Flick, Tobias and Flierl, Bernhard Matthias and Flores, Carlos and Flores, Lucas Macrorie and Flores, Marvin and Flores Castillo, Luis Roberto and Follega, Francesco Maria and Fomichev, Ivan and Fomin, Nikolai and Fontaine, Michel and Foo, Joel Hengwei and Forland, Blake Christopher and Formenti, Lia and Formica, Andrea and Forti, Alessandra and Fortin, Etienne Marie and Fortman, Anne Winifred and Foti, Maria Giovanna and Fougeron, Denis and Fountas, Leonidas and Fournier, Daniel and Fox, Harald and Fragnaud, Jasmin Corentin and Francavilla, Paolo and Francescato, Simone and Franchellucci, Stefano and Franchini, Matteo and Franchino, Silvia and Francis, David and Franco, Luca and Franconi, Laura and Frank, Norbert and Franklin, Melissa and Fras, Markus and Frattari, Guglielmo and Freddi, Angelo and Freegard, Arran Charles and Freeman, Patrick Moriishi and Spolidoro Freund, Werner and Fritzsche, Nick and Froch, Alexander and Froidevaux, Daniel and Frost, James and Fu, Yao and Fujimoto, Minori and Fullana Torregrosa, Esteban and Furukawa, Marin and Fuster Verdu, Juan and Gabrielli, Alessandro and Gabrielli, Andrea and Gadow, Philipp and Gagliardi, Guido and Gagnon, Louis-Guillaume and Gallardo, Gabriel and Gallas, Elizabeth and Galleguillos Silva, Renato Bruno and Gallop, Bruce Joseph and Galuszka, Szymon Jan and Gamboa Goni, Rodrigo and Gan, Kock Kiam and Ganguly, Sanmay and Gantel, Laurent Mathieu and Gao, Jun and Gao, Wei and Gao, Yanyan and Garay Walls, Francisca and Garcia, Berenice and Garcia, Carmen and Garcia Navarro, Jose Enrique and Garcia-Sciveres, Maurice and Gardner, Robert William, Jr. and Gareau, Matthew and Garelli, Nicoletta and Garg, Diksha and Garg, Rocky Bala and Gariano, Giuseppe and Garner, Christopher Andrew and Garonne, Vincent and Gasiorowski, Sean Joseph and Do Nascimento Gaspar, Philipp and Gaudiello, Andrea and Gaudio, Gabriella and Gautam, Viveka and Gauzzi, Paolo and Gavrilenko, Igor and Gavriliuk, Alexander and Gay, Colin Warren and Gaycken, Goetz and Gazis, Evangelos and Geanta, Andrei Alexandru and Gebyehu, Mesfin and Gee, Carolyn and Gee, Norman and Geisen, Jannik and Gemme, Claudia and Genest, Marie-Helene and Gentile, Simonetta and George, Matthias Alexander and George, Simon and George, William Frederick and Georgiev, Vjaceslav and Geralis, Theodoros and Gerlach, Lino Oscar and Gessinger-Befurt, Paul and Ghasemi Bostanabad, Meisam and Ghneimat, Mazuza and Ghorbanian, Keanu and Ghosal, Arpan and Ghosh, Aishik and Ghosh, Anindya and Giacobbe, Benedetto and Giagu, Stefano and Giannetti, Paola and Giannini, Antonio and Gibson, Stephen and Giganon, Giganon and Gigliotti, Kade and Gignac, Matthew and Gil, Damian Tomasz and Gilbert, Alexander Kevin and Gilbert, Benjamin Jacob and Gillberg, Dag and Gilles, Geoffrey and Gillwald, Nils and Ginabat, Louis and Gingrich, Doug and Giokaris, Athanasios and Giordani, Mapo and Giraud, Julien and Giraud, Pierre-Francois and Giromini, Paolo and Giugliarelli, Gilberto and Giugni, Danilo and Giuli, Francesco and Gjersdal, Havard Ervik and Gkialas, Ioannis and Gkougkousis, Vagelis and Gkountoumis, Panagiotis and Gladilin, Leonid and Glasman, Claudia and Gledhill, Galen Rhodes and Glisic, Marija and Glitza, Karl-Walter and Glonti, George and Gnesi, Ivan and Go, Yeonju and Goblin, Cyril and Goblirsch-Kolb, Maximilian Emanuel and Gocke, Benedikt and Godin, Dominique and Godiot, Stephanie and Gokturk, Berare and Goldfarb, Steven and Golling, Tobias and Gololo, Mpho Gift Doctor and Golubkov, Dmitry and Golyzniak, Dominik and Gombas, Jason Peter and Da Silva Gomes, Agostinho and Gomes, Gabriel and Gomez Delegido, Antonio Jesus and Goncalves Gama, Rafael and Morais Silva Goncalo, Ricardo Jose and Gonella, Giulia and Gonella, Laura and Gong, Datao and Gongadze, Alexi and Gonnella, Francesco and Gonski, Julia Lynne and Gonzalez, Richards and Gonzalez De La Hoz, Santiago and Gonzalez Fernandez, Sergio and Gonzalez Lopez, Ricardo and Gonzalez Renteria, Cesar and Gonzalez Suarez, Rebeca and Gonzalez Sevilla, Sergio and Gonzalvo Rodriguez, Galo Rafael and Goossens, Luc and Gorasia, Nandish Arjan and Gorbounov, Petr and Gorini, Benedetto and Gorini, Edoardo and Gorisek, Andrej and Goshaw, Al and Goessling, Claus and Gostkin, Mikhail and Goswami, Soumyananda and Gottardo, Carlo Alberto and Gotz, Stefanie Andrea and Gouighri, Mohamed and Goumarre, Vincent and Goussiou, Anna and Govender, Nicolin and Goy, Corinne and Grabas, Aude Marie and Grabowska-Bold, Iwona and Graham, Kevin Robert and Gramstad, Eirik and Grancagnolo, Sergio and Grandi, Mario and Grachev, Vadim and Gravelle, Philippe and Gravila, Paul and Gravili, Francesco Giuseppe and Gray, Heather and Grayzman, Israel and Greco, Matteo and Grefe, Christian and Gregor, Ingrid and Grenier, Philippe and Grieco, Chiara and Grillo, Alex and Grimm, Kathryn and Grinstein, Sebastian and Grivaz, Jean-Francois and Grohs, Johannes Philipp and Gross, Eilam and Grosse-Knetter, Joern and Grud, Christopher Ryan and Grummer, Aidan and Grundy, James Cameron and Guan, Liang and Guan, Wen and Gubbels, Chris and Guelfo Gigli, Samuel and Guerrero Rojas, Jesus and Guerrieri, Giovanni and Guescini, Francesco and Guettouche, Nour El Houda and Gugel, Ralf and Guhit, Jem Aizen Mendiola and Guida, Alessandro and Guido, Elisa and Guillard, Jean-Christophe and Guillemin, Thibault and Guilloton, Eva and Guindon, Stefan and Guo, Di and Guo, Fangyi and Guo, Jun and Guo, Linghua and Guo, Yuxiang and Gupta, Ruchi and Gurbuz, Saime and Gurdasani, Simran Sunil and Gustavino, Giuliano and Guth, Manuel and Gutierrez, Phillip and Gutierrez Zagazeta, Luis Felipe and Gutschow, Christian and Guyot, Claude and Gwenlan, Claire and Gwilliam, Carl and Haaland, Even Simonsen and Haas, Andrew and Haas, Stefan Ludwig and Habedank, Martin and Haber, Carl and Habring, Jorg and Hadavand, Haleh and Hadef, Asma and Hadzic, Sejla and Haines, Emil and Haleem, Mahsana and Haley, Joseph and Hall, Jack Joseph and Hallewell, Gregory and Halser, Lea and Hamano, Kenji and Hamdaoui, Hassane and Hamer, Matthias and Hamity, Guillermo Nicolas and Han, Jingyi and Han, Kunlin and Han, Liangliang and Han, Liang and Han, Shuo and Han, Yi Fei and Hanagaki, Kazunori and Hance, Mike and Hangal, Dhanush Anil and Hanif, Hamza and Hank, Michael Donald and Hankache, Robert and Hansen, Jorgen Beck and Hansen, Dines and Hansen, Peter and Hara, Kazuhiko and Harada, Daigo and Harenberg, Torsten and Harkusha, Siarhei and Harris, Ynyr and Harrison, Natalie and Harrison, Paul Fraser and Hartman, Nicole Michelle and Hartmann, Nikolai and Hartung, Peter and Hasegawa, Yoji and Hashemi, Kevan and Hasib, Ahmed and Hasley, Lloyd Alan and Haubold, Thomas and Haug, Sigve and Hauser, Reiner and Havranek, Miroslav and Hawkes, Chris and Hawkings, Richard and Hayashi, Yuichiro and Hayashida, Shota and Hayden, Daniel and Hayes, Christopher Robyn and Hayes, Robin and Hays, Chris and Hays, Jonathan and Hayward, Helen and He, Fudong and He, Yunjian and He, Yajun and Heath, Matthew Peter and Hedberg, Vincent and Heggelund, Andreas Lokken and Hehir, Natasha and Heidegger, Constantin and Heidegger, Kim Katrin and Heidorn, William Dale and Heilman, Jesse and Heim, Sarah and Heim, Timon and Heinlein, James Geddy and Heinrich, Jochen Jens and Heinrich, Lukas Alexander and Hejbal, Jiri and Helary, Louis and Held, Alexander and Hellesund, Simen and Helling, Cole Michael and Hellman, Sten and Helsens, Clement and Hemperek, Tomasz and Henderson, Robert and Henkelmann, Lars and Henriques Correia, Ana Maria and Hentges, Rainer and Herde, Hannah Elizabeth and Hernandez Jimenez, Yesenia and Herrmann, Lena Maria and Herrmann, Maximilian Georg and Herrmann, Tim and Herten, Gregor and Hertenberger, Ralf and Hervas, Luis and Hessey, Nigel and Hibi, Hiroaki and Higon-Rodriguez, Emilio and Hillier, Stephen and Hils, Maximilian and Hinchliffe, Ian and Hinterkeuser, Florian and Hirose, Minoru and Hirose, Shigeki and Hirschbuehl, Dominic and Hitchings, Thomas Glyn and Hiti, Bojan and Hobbs, John David and Hobincu, Radu and Tal Hod, Noam and Hodgkinson, Mark and Hodkinson, Benjamin Haslum and Hoecker, Andreas and Hoeferkamp, Martin and Hofer, Judith and Hoffmann, August Edward and Hoffmann, Dirk and Hohn, David and Hohov, Dmytro and Holm, Tanja and Holzbock, Michael and Hommels, Bart and Honan, Benjamin Paul and Hong, Jiangliu and Hong, Tae Min and Honig, Jan Cedric and Honle, Andreas and Hooberman, Benjamin Henry and Hopkins, Walter and Horii, Yasuyuki and Horn, Philipp and Hou, Suen and Howard, Alissa Shirley-Ann and Howarth, James William and Hoya, Joaquin and Hrabovsky, Miroslav and Hrynevich, Aliaksei and Hrynova, Tetiana and Hsu, Pai-Hsien and Hsu, Shih-Chieh and Hu, Kun and Hu, Qipeng and Hu, Xueye and Hu, Yifan and Huang, Danping and Huang, Fred and Huang, Shuhui and Huang, Xiaozhong and Huang, Yicong and Huang, Yanping and Huang, Zuchen and Hubacek, Zdenek and Hubner, Michael and Huegging, Fabian and Huffman, Todd Brian and Huhtinen, Mika and Huiberts, Simon Kristian and Hulek, Wojciech Krzysztof and Hulsken, Raphael Alain and Huseynov, Nazim and Huston, Joey and Huth, John and Hyneman, Rachel Jordan and Hyrych, Sofiia and Iacobucci, Giuseppe and Iakovidis, George and Iakovidis, Kostas and Iankovskaia, Vera and Yankovsky, Boris and Ibragimov, Iskander and Iconomidou-Fayard, Lydia and Iengo, Paolo and Iguchi, Ryunosuke and Iizawa, Tomoya and Ikegami, Yoichi and Ikeno, Masahiro and Ilg, Armin and Ilic, Nikolina and Ilyashenko, Igor and Imam, Hajar and Ingebretsen Carlson, Tom and Innocente, Angelo and Introzzi, Gianluca and Iodice, Mauro and Ippolito, Valerio and Ishino, Masaya and Islam, Wasikul and Issever, Cigdem and Issinski, Sergey and Istin, Serhat and Ito, Hiroki and Iturbe, Julia and Iuppa, Roberto and Iurikovskii, Oleg and Ivanovici, Mihail and Ivina, Anna and Izen, Joseph Michael and Izzo, Vincenzo and Jacka, Petr and Jackson, Paul and Jacobs, Ruth Magdalena and Jaeger, Benjamin Paul and Jagfeld, Christoph and Jain, Prasham and Jakel, Gunnar and Jakobs, Karl and Jakoubek, Tomas and Jamieson, Jonathan and Janas, Krzysztof and Janssen, Jens and Jarlskog, Goran and Jaspan, Adam Elliott and Javello, Sandrine and Javurkova, Martina and Jeanneau, Fabien and Jeanty, Laura and Jejelava, Juansher and Jenni, Peter and Mertens, Jennifer and Jessiman, Callan and Jezequel, Stephane and Jia, Chen and Jia, Jiangyong and Jia, Xiaohe and Jia, Xuewei and Jia, Zihang and Jiang, Yi and Jiggins, Stephen and Jimenez Pena, Javier and Jin, Ge and Jin, Shan and Jinaru, Adam and Jinnouchi, Osamu and Johansson, Per Daniel Conny and Johns, Kenneth and Johnson, Jacob Wayne and Jones, Dominic and Jones, Eleanor and Jones, Paul and Jones, Roger and Jones, Tim and Joos, Markus and Joseph, John Matthew and Joshi, Roshan and Jourde, Didier and Jovicevic, Jelena and Ju, Xiangyang and Junggeburth, Johannes and Junkermann, Thomas and Juste Rozas, Aurelio and Kabana, Sonia and Kaczmarska, Anna and Kado, Marumi and Kagan, Harris and Kagan, Michael and Kahn, Alan Mathew and Kahn, Abraham and Kahra, Christian and Kaji, Toshiaki and Kajomovitz Must, Enrique and Kakati, Nilotpal and Kalderon, Will and Kallitsopoulou, Alexandra and Kamenshchikov, Andrey and Kanayama, Shuhei and Kanellos, Nikolaos and Kang, Nathan Jihoon and Kapusciak, Kacper Kamil and Kar, Deepak and Karava, Kla and Kareem, Mohammad and Karentzos, Efstathios and Karkanias, Ioannis and Karpov, Sergey and Karpova, Zoya and Kartvelishvili, Vato and Karyukhin, Andrey and Kasimi, Eirini and Kato, Chikuma and Katunin, Serge and Katzy, Judith and Kaur, Sandeep and Kawade, Kentaro and Kawagoe, Kiyotomo and Kawamoto, Tatsuo and Kawamura, Gen and Kay, Ellis and Kaya, Colette and Kazakos, Stergios and Kazanin, Vassili and Kazarov, Andrei and Ke, Yan and Keaveney, James Michael and Keberri, Mariam and Keeler, Richard and Kehris, Gustavs and Keller, John Stakely and Kelly, Aidan Sean and Kelsey, Daniel Christopher and Kempster, Jacob Julian and Kennedy, Kiley Elizabeth and Kennedy, Philip David and Kepka, Oldrich and Kerridge, Benjamin Philip and Kersten, Susanne and Kersevan, Borut Paul and Keshri, Sumit and Keszeghova, Lucia and Ketabchi, Sana and Khandoga, Mykola and Khanov, Alexander and Kharlamov, Alexey and Kharlamova, Tatyana and Khoda, Elham E. and Khoo, Teng Jian and Khoriauli, Gia and Khubua, Djemal and Khwaira, Yahya A.R. and Kiehn, Moritz and Kilgallon, Aaron Joseph and Kim, Dongwon and Kim, Eunchong and Kim, Young-Kee and Kimura, Naoki and Kind, Peter and Kinget, Peter and Kirchhoff, Andreas and Kirchmeier, David and Kirfel, Christian and Kirk, Julie and Kiryunin, Andrei and Kishimoto, Tomoe and Kiskiras, Ioannis and Kisliuk, Dylan Perry and Kitsaki, Chara and Kivernyk, Oleh and Klapdor-Kleingrothaus, Thorwald and Klassen, Martin and Klein, Christoph Thomas and Klein, Lucas and Klein, Matthew Henry and Klein, Max and Klein, Samuel Byrne and Klein, Uta and Klemm, Peter Rudolf and Klimek, Pawel Jan and Klimentov, Alexei and Klimpel, Fabian and Klioutchnikova, Tatiana and Klitzner, Felix Fidelio and Kluit, Peter and Kluth, Stefan and Kneringer, Emmerich and Knight, Timothy Michael and Knoops, Edith and Knue, Andrea Helen and Kobayashi, Dai and Kobayashi, Ren and Kocian, Martin and Kodys, Peter and Kock, Daniela and Konig, Philipp and Koffas, Thomas and Kolb, Mathis and Kolbasin, Andrey and Koletsou, Iro and Kolitsi, Foteini and Komarek, Tomas and Kompogiannis, Spyridon and Koeneke, Karsten and Kong, Albert and Kongsore, Marius and Kono, Takanori and Konstantinidis, Nikolaos and Konya, Balazs and Kopeliansky, Revital and Koperny, Stefan Zenon and Korcyl, Krzysztof Marian and Kordas, Kostas and Koren, Guy and Korn, Andreas and Korn, Steffen and Korolkov, Ilya and Korotkova, Natalia and Kortman, Bryan and Kortner, Oliver and Kortner, Sandra and Kostecka, Will and Kostyukhin, Vadim and Kotsokechagia, Anastasia and Kotwal, Ashutosh and Koulouris, Aimilianos and Kourkoumeli-Charalampidi, Athina and Kourkoumelis, Christine and Kourlitis, Vangelis and Koutelieris, Georgios and Koutsosimos, Theodoros and Kouyoumdjian Argueso, David Felipe and Kovalenko, Serguei and Kovanda, Ondrej and Koveshnikov, Ilya and Kowalewski, Bob and Kozanecki, Witold and Kozhin, Anatoli and Kramarenko, Viktor and Kramberger, Gregor and Kramer, Peter and Krasny, Mieczyslaw Witold and Krasznahorkay, Attila and Kremer, Jakub and Kresse, Tom and Kretzschmar, Jan and Kreul, Ken Matthias and Krieger, Nina and Krieger, Peter and Krishnamurthy, Samyukta and Krivos, Martin and Krizka, Karol and Kroeninger, Kevin Alexander and Kroha, Hubert and Kroll, Jiri and Kroll, Ira Joseph and Krowpman, Kyle Stuart and Kruchonak, Uladzimir and Krueger, Hans and Krumnack, Nils Erik and Kruse, Mark and Krzysiak, Janina Anna and Kubik, Zdenek and Kuchinskaia, Olesia and Kuday, Sinan and Kuchler, Daniela and Kuechler, Jan and Kuehn, Susanne and Kusters, Roman and Kuger, Fabian and Kuhl, Thorsten and Kukhtin, Victor and Koultchitski, Yuri and Kuleshov, Serguei and Kulinska, Anna Malgorzata and Kulka, Peter and Kumar, Mukesh and Kumari, Neelam and Kunkler, Brandon Michael and Kupco, Alexander and Kupfer, Tobias and Kupich, Andrey and Kuppambatti, Jayanth and Kuprash, Oleg and Kurashige, Hisaya and Kurchaninov, Leonid and Kurochkin, Yurii and Kurova, Anastasia and Kuze, Masahiro and Kvam, Audrey Katherine and Kvita, Jiri and Kwan, Tony and Kwok, King Wai and Kyriacou, Nicholas and Kyriakis-Bitzaros, Efstathios and Laatu, Lauri Antti Olavi and Lacasta Llacer, Carlos and Lacava, Francesco and Lacker, Heiko Markus and Lacour, Didier and Lad, Nisha and Ladygin, Evgueni and Laforge, Bertrand and Lafrasse, Sylvain and Lagouri, Theodota and Lai, Stan and Lakomiec, Inga Katarzyna and Lalloue, Nathan and Lam, Hon Piu and Lambert, Joseph Earl and Lammers, Sabine Wedam and Lampardaki, Eleni and Lampl, Walter and Lampoudis, Christos and Lancaster, Alec and Lancon, Eric Christian and Landgraf, Ulrich and Landon, Murrough and Landraud, Cedric and Lang, Valerie and Langenberg, Robert Johannes and Langstaff, Roy and Lankford, Andrew James and Lanni, Francesco and Lantzsch, Kerstin and Lanza, Agostino and Lapertosa, Alessandro and Laporte, Didier and Laporte, Jean-Francois and Lari, Tommaso and Lasagni Manghi, Federico and Lassnig, Mario and Latonova, Vera and Latorre, Stefano and Lau, Tak Shun and Laudrain, Antoine and Laugier, Daniele and Laurier, Alexandre and Lawlor, Sean Dean and Lawrence, Zak and Lax, Ignazio and Lazzaroni, Massimo and Le, Brian and Le Goff, Fabrice and Le, Xiao and Le Bourlout, Pascal and Leban, Blaz and Lebedev, Alexandre and Leblanc, Matt and Leboeuf, Didier and Leboube, Christian Georges and Le Compte, Tom and Ledroit, Fabienne and Lee, Ava Chloe Audrey and Lee, Claire and Lee, Graham Richard and Lee, Lawrence, Jr. and Lee, Shih-Chang and Lee, Suhyun and Lee, Ting Fung and Leeuw, Lerothodi Leonard and Lefebvre, Benoit and Lefebvre, Helena and Lefebvre, Michel and Lefevre, Marc and Leggett, Charles and Lehmann, Konstantin and Lehmann Miotto, Giovanna and Leigh, Matthew and Leight, William Axel and Leis, Ulrich and Leisos, Antonios and Lisboa Leite, Marco and Leitgeb, Clara Elisabeth and Leitner, Rupert and Lenckowski, Mark and Leney, Katharine and Lenz, Tatjana and Leone, Sandra and Leonidopoulos, Christos and Leopold, Alexander and Lepota, Thabo James and Leroy, Claude and Les, Robert and Lester, Christopher and Leung, Hin Kwong and Levchenko, Mikhail and Leveque, Jessica and Levin, Dan and Levinson, Lorne and Lewicki, Maciej Piotr and Lewis, Daniel and Li, Ang and Li, Bing and Li, Chihao and Li, Changyu and Li, Changqiao and Li, Feng and Li, Heng and Li, Han and Li, Huanguo and Li, Haifeng and Li, Jing and Li, Ke and Li, Liang and Li, Mengran and Li, Quanyin and Li, Shuqi and Li, Shu and Li, Tong and Li, Xingguo and Li, Xiaoting and Li, Zhi and Li, Zhiying and Li, Zhelun and Li, Zhiyuan and Li, Zhan and Liang, Futian and Liang, Zhijun and Liaqat, Shawaiz and Liberatore, Marianna and Liberti, Barbara and Lie, Ki and Lieber Marin, Juan and Lien, Hsuan-Chu and Lim, Chapman and Lin, Kuan-Yu and Lin, Tai-Hua and Linck, Rebecca and Lindley, Rachel Elizabeth and Lindon, Jack and Linss, Arthur and Lipeles, Elliot and Lipniacka, Anna and Lippert, Markus and Lister, Alison and Little, Jared and Liu, Bo and Liu, Bingxuan and Liu, Chonghan and Liu, Danning and Liu, Hongbin and Liu, Jianbei and Liu, Jesse and Liu, Kun and Liu, Minghui and Liu, Mingyi and Liu, Peilian and Liu, Qibin and Liu, Shengquan and Liu, Tiankuan and Liu, Xiaotian and Liu, Yang and Liu, Yanlin and Liu, Yanwen and Liubimtcev, Dmitrii and Livan, Michele and Liz Vargas, Matias Patricio and Llorente Merino, Javier and Lloyd, Steve and Lobodzinska, Ewelina Maria and Loch, Peter and Loffredo, Salvatore and Lohse, Thomas and Lohwasser, Kristin and Loiseau, Carl Arthur and Lokajicek, Milos and Long, Jonathan and Longarini, Iacopo and Longo, Luigi and Longo, Riccardo and Lopez Mateos, David and Lopez Paz, Ivan and Lopez Solis, Alvaro and Lorenz, Jeanette Miriam and Lorenzo Martinez, Narei and Lory, Alexander and Losel, Philipp Jonathan and Lou, Xuanhong and Lou, Xinchou and Lounis, Abdenour and Love, Jeremy and Love, Peter and Lozano Bahilo, Julio and Lu, Gangcheng and Lu, Miaoran and Lu, Sicong and Lu, Yun-Ju and Lubatti, Henry and Luci, Claudio and Lucio Alves, Fabio Lucio and Lucotte, Arnaud and Luehring, Fred and Luise, Ilaria and Lukianchuk, Oleksii and Luminari, Lamberto and Lunadei, Riccardo and Lundberg, Olof and Lund-Jensen, Bengt and Luongo, Nicholas and Lupu, Nachman and Lutz, Margaret Susan and Luz, Ricardo and Lynn, David and Lyons, Harry John and Lysak, Roman and Lytken, Else and Lyu, Feng and Lyubushkin, Vladimir and Lyubushkina, Tatiana and Lyukova, Mars and Ma, Hong and Ma, Lianliang and Ma, Yanhui and Macdonell, Danika Marina and Maccarrone, Giovanni and Macdonald, Jack and Madar, Romain and Mader, Wolfgang and Madhoun, Khaled and Maeda, Junpei and Maeno, Tadashi and Maerker, Max and Maguire, Helen Ruth and Mahon, Devin and Maier, Ronald and Maio, Amelia and Maj, Klaudia and Majersky, Oliver and Majewski, Stephanie and Makovec, Nikola and Maksimovic, Veljko and Malaescu, Bogdan and Malaquin, Joffrey Alex and Malecki, Pawel and Maleev, Victor and Malek, Fairouz and Malito, Davide and Mallik, Usha and Malone, Claire and Maltezos, Stavros and Maly, Pavel and Malyukov, Sergey and Mamuzic, Judita and Manca, Fabio Oscar and Mancini, Giada and Manco, Giulia and Mandalia, Jesal and Mandic, Igor and Mandjavidze, Irakli and Manhaes De Andrade Filho, Luciano and Maniatis, Ioannis Michail and Manisha, Manisha and Manjarres, Joany and Mankad, Dvij and Mann, Alexander and Manousos, Thanos and Manson, Sebastian and Mansoulie, Bruno and Manthos, Ioannis and Manzoni, Stefano and Maragkou Didi, Evaggelia and Marantis, Alexandros and Marchiori, Giovanni and Marcisovsky, Michal and Marcon, Caterina and Marinescu, Mihaela and Marjanovic, Marija and Marshall, Emma and Marshall, Zach and Marti I Garcia, Salvador and Martin, Tim and Martin, Victoria and Martin Dit Latour, Bertrand and Martinelli, Luca and Martinez-Perez, Mario and Martinez Agullo, Pablo and Martinez Outschoorn, Verena Ingrid and Martinez Suarez, Paula and Martin-Haugh, Stewart and Martoiu, Sorin and Martyniuk, Alex Christopher and Marzin, Antoine and Mas, Philippe and Maschek, Stefan Raimund and Mascione, Daniela and Masetti, Lucia and Mashimo, Tetsuro and Masik, Jiri and Maslennikov, Alexei and Massa, Lorenzo and Massarotti, Paolo and Massol, Nicolas and Mastrandrea, Paolo and Mastroberardino, Anna and Masubuchi, Tatsuya and Matakias, Dimitrios and Mathisen, Thomas and Matsuzawa, Nobuo and Mattig, Peter and Maurer, Julien and Macek, Bostjan and Maximov, Dmitriy and Mazini, Rachid and Maznas, Ioannis and Mazza, Maria and Mazza, Simone Michele and Mc Ginn, Christopher and Mc Gowan, John Patrick and Mc Kee, Shawn and Mccracken, Callum and Mcdonald, Millie and Mcdougall, Ashley Ellen and Mcfayden, Josh and Mc Govern, Bobby and Mchedlidze, Gvantsa and Mckenzie, Ryan Peter and Mclachlan, Thomas Christopher and Mclaughlin, Donal Joseph and Mclean, Kayla and Mcmahon, Steve and Mcnamara, Peter Charles and Mcpartland, Conor Michael and Mcpherson, Rob and Megy, Theo and Mehalev, Ifat and Mehlhase, Sascha and Mehta, Andrew and Meirose, Bernhard and Melini, Davide and Mellado Garcia, Bruce and Melo, Andres Hugo and Meloni, Federico and Gouveia, Emanuel and Mendes Jacques Da Costa, Antonio Manuel and Meneghini, Stefano and Meng, Huan Yu and Meng, Lingxin and Menke, Sven and Menouni, Mohsine and Mentink, Matthias and Meoni, Evelin and Merlassino, Claudia and Merola, Leonardo and Meroni, Chiara and Merz, Garrett William and Meshkov, Oleg and Mesolongitis, Ioannis and Metcalfe, Jessica and Mete, Alaettin Serhan and Meyer, Chris and Meyer, Jean-Pierre and Miao, Peng and Miccoli, Alessandro and Michal, Sebastien and Michetti, Michele and Middleton, Robin and Miglioranzi, Silvia and Migne, Julien and Mijovic, Liza and Mikenberg, George and Mikestikova, Marcela and Mikuz, Marko and Mildner, Hannes and Milic, Adriana and Milke, Chris and Miller, David and Miller, Laura Stephanie and Milov, Alexander and Milovanovic, Marko and Milstead, David Anthony and Min, Tianjue and Minaenko, Andrei and Minami, Yuto and Minano Moya, Mercedes and Minashvili, Irakli and Mince, Laurynas and Mincer, Allen Irving and Mindur, Bartosz and Mineev, Mikhail and Mino, Yuya and Mir Martinez, Lluisa Maria and Miralles Lopez, Marcos and Mironova, Maria and Missio, Marion and Mitani, Takashi and Mitra, Ankush and Mitsou, Vasiliki and Miu, Ovidiu and Miyagawa, Paul and Miyazaki, Yuta and Mizukami, Atsushi and Mjoernmark, Jan-Ulf and Mkrtchyan, Tigran and Mladenovic, Goran and Mlinarevic, Toni and Mlynarikova, Michaela and Moa, Torbjorn and Mobius, Silke and Mochizuki, Kazuya and Moder, Paul and Mogg, Philipp and Fadol Mohammed, Abdualazem and Mohapatra, Soumya and Mokgatitswane, Gaogalalwe and Mokrenko, Sergei and Moleri, Luca and Molina Gonzalez, Emmanuel Josue and Monay E Silva, Alessa and Mondal, Buddhadeb and Mondal, Santu and Monig, Klaus and Monnier, Emmanuel and Monsonis Romero, Luis and Montejo Berlingen, Javier and Montella, Marco and Monti, Mauro and Monticelli, Fernando and Moraga Jimenez, Juan Pablo Andres and Morange, Nicolas and Carvalho, Luisa and Moreno Llacer, Maria and Moreno Martinez, Carlos and Morettini, Paolo and Morgenstern, Stefanie and Morii, Masahiro and Morinaga, Masahiro and Morley, Anthony and Morodei, Federico and Morvaj, Ljiljana and Moschovakos, Paris and Moser, Brian and Mosidze, Maia and Moskalets, Tetiana and Moskvitina, Polina and Moss, Joshua and Moszkowicz, Piotr and Moyse, Edward and Mtintsilana, Onesimo and Muanza, Steve and Mueller, James Alfred and Muller, Ralph and Muenstermann, Daniel and Mueller, Roman and Mullier, Geoffrey and Mullin, Joseph and Mulski, Alexis Elizabeth and Mungo, Davide Pietro and Munoz Martinez, Jose Luis and Munoz Perez, David and Munoz Sanchez, Francisca and Mur, Michel Jean Yves and Murin, Martin and Murray, Bill and Murrone, Alessia and Muse, Joseph M. and Muskinja, Miha and Mwewa, Chilufya and Myagkov, Alexei and Myers, Andrew Joel and Myers, Ava Anne and Myers, Greg and Myska, Miroslav and Nachman, Ben and Nackenhorst, Olaf and Naeem, Muhammad and Nag, Abhishek and Nagai, Koichi and Nagano, Kunihiro and Nagle, James Lawrence and Nagy, Elemer and Nairz, Armin and Nakahama Higuchi, Yu and Nakamura, Koji and Nanjo, Hajime and Narayan, Rohin Thampilali and Narayanan, Easwar Anand and Narevicius, Julia and Narvaez Paredes, Lautaro Leon and Naryshkin, Iurii and Naseri, Mohsen and Nass, Christian and Natsios, Marios Dimitrios and Navarro, Gabriela Alejandra and Navarro Gonzalez, Josep and Nayak, Ranjit and Nayaz, Ab and Nechaeva, Polina and Nechansky, Filip and Nedic, Luka and Neep, Tom and Negri, Andrea and Negrini, Matteo and Nellist, Clara and Nelson, Christina and Nelson, Kevin Michael and Nemecek, Stanislav and Nessi, Marzio and Neubauer, Mark and Neuhaus, Friedemann and Neundorf, Jonas and Newhouse, Robin and Newman, Paul Richard and Ng, Chi Wing and Ng, Sam Yanwing and Ng, Ying Wun Yvonne and Ngair, Badr-Eddine and Nguyen, Hoang Dai Nghia and Nickerson, Richard and Nikolaidou, Rosy and Nielsen, Jason and Niemeyer, Marcel and Nikiforou, Nikiforos and Nikolaenko, Vladimir and Nikolic, Irena and Nikolopoulos, Konstantinos and Nila, Michael Lee and Nilsson, Paul and Ninca, Ilona-Stefana and Nindhito, Herjuno Rah and Nisati, Aleandro and Nishu, Nishu and Nisius, Richard and Nitschke, Jan-Eric and Nkadimeng, Edward Khomotso and Noacco Rosende, Santiago and Nobe, Takuya and Noel, Daniel Louis and Noel, Jerome and Noguchi, Yohei and Nommensen, Thomas and Nomura, Marcelo Ayumu and Norfolk, Mitchell Bradley and Bin Norisam, Raif Rafideen and Norman, Bryce John and Novak, Jakob and Novak, Tadej and Novgorodova, Olga and Novotny, Lukas and Novotny, Radek and Nozka, Libor and Ntekas, Kostas and Nunes De Moura, Natanael, Junior. and Nurse, Emily Laura and Oakham, Gerald and Ocariz, Jose Humberto and Ochi, Atsuhiko and Ochoa, Ines and Ockenfels, Walter and Oehm, Rolf and Ordek, Serhat and Offermann, Jan Tuzlic and Ogrodnik, Agnieszka Ewa and Oh, Alexander and Ohm, Christian and Oide, Hideyuki and Oikonomou, Konstantinos and Oishi, Reiyo and Ojeda, Martina Laura and Okazaki, Yuta and O'Keefe, Michael William and Okumura, Yasuyuki and Olariu, Albert and Seabra, Luis and Olivares, Sebastian and Oliveira Damazio, Denis and Oliveira Goncalves, Dayane and Oliver, Jason and Olsson, Mats Joakim Robert and Olszewski, Andrzej and Olszowska, Jolanta and Oncel, Ogul and O'Neil, Dugan and O'Neill, Aaron Paul and Onofre, Antonio and Onyisi, Peter and Openshaw, Robert and Oreglia, Mark and Orellana, Gonzalo Enrique and Orestano, Domizia and Orlando, Nicola and Orr, Robert and O'Shea, Val and Ospanov, Rustem and Ostrega, Maciej Stanislaw and Otero Y Garzon, Gustavo and Otono, Hidetoshi and Ott, Philipp Sebastian and Ottino, Gregory James and Ouchrif, Mohamed and Ouellette, Jeff and Ould-Saada, Farid and Owen, Mark Andrew and Owen, Rhys and Oyulmaz, Kaan Yuksel and Ozbey, Aydin and Ozcan, Erkcan and Ozturk, Nurcan and Ozturk, Sertac and Pacalt, Josef and Pacey, Holly and Pachal, Katherine and Pacheco Pages, Andreu and Padilla Aranda, Cristobal and Padovano, Giovanni and Pagan Griso, Simone and Palacino, Gabriel and Palazzo, Alessandra and Palestini, Sandro and Palka, Marek and Pan, Jingjing and Pan, Tong and Pancake, Charles and Panchal, Dev and Pandini, Carlo Enrico and Panduro Vazquez, William and Pang, Hao and Pangaud, Patrick and Pani, Priscilla and Panico, Lorenzo and Panizzo, Giancarlo and Paolozzi, Lorenzo and Papadatos, Constantine and Parajuli, Santosh and Paramonov, Alexander and Paraskevopoulos, Christos and Paredes Hernandez, Daniela Katherinne and Park, Tae Hyoun and Parker, Andy and Parodi, Fabrizio and Parrish, Elliot and Parrish, Victoria Alexis and Parsons, John and Paruzza, Gianfranco and Parzefall, Ulrich and Paschalias, Panagiotis and Pascual Dias, Bruna and Pascual Dominguez, Luis and Pascuzzi, Vincent and Pasmantirer, Binyamin and Pasquali, Federica and Pasqualucci, Enrico and Passaggio, Stefano and Pastore, Francesca and Pastori, Enrico and Pasuwan, Patrawan and Patel, Pragati and Pater, Jo and Pauly, Thilo and Pazos, Camila and Pearkes, Jannicke Andree and Pedersen, Maiken and Costa Batalha Pedro, Rute and Peleganchuk, Sergey and Pelosi, Alessandro and Penc, Ondrej and Pender, Emily Alexandra and Peng, Chen and Peng, Haiping and Penski, Katrin Elisabeth and Penzin, Maksim and Pepe, Marco and Sotto-Maior Peralva, Bernardo and Pereira, Mackenzie and Peixoto, Ana and Pereira Sanchez, Laura and Perepelitsa, Dennis and Perez Codina, Estel and Perez Gomez, Francisco and Perganti, Maria and Perini, Laura and Pernegger, Heinz and Perrella, Sabrina and Perrevoort, Ann-Kathrin and Perrin, Oceane and Perrot, Guy and Peters, Krisztian and Peters, Reinhild and Petersen, Brian and Petersen, Troels and Petit, Elisabeth and Petousis, Vlasios and Petridou, Chariclia and Petruccetti, Marco and Petrucci, Fabrizio and Petrukhin, Alexey and Pettee, Mariel and Pettersson, Nora Emilia and Petukhov, Aleksandr and Mihule, Kristina and Peyaud, Alan and Pezoa Rivera, Raquel and Pezzotti, Lorenzo and Pezzullo, Gianantonio and Pfeifer, Bernhard and Pham, Minh Tuan and Pham, Joni and Phillips, Peter and Phipps, Michael William and Piacquadio, Giacinto and Pianori, Elisabetta and Piazza, Federica and Piegaia, Ricardo and Pietreanu, Dorel and Pileggi, Giuseppe and Pilkington, Andrew and Pinamonti, Michele and Pinfold, James and Pereira, Beatriz and Pinnell, Jamie Thomas and Pinto Bustos, Roberto Andres and Pirea, Radu Nicolae and Piret, Yves and Pirola, Michele and Piscitelli, Francesco and Pitman Donaldson, Charlie Bruno and Pizzi, Dylan and Pizzimento, Luca and Pizzini, Alessio and Pleier, Marc-Andre and Plesanovs, Vladislavs and Pleskot, Vojtech and Plotnikova, Elena and Pluzhnikov, Andrey and Poddar, Gitanjali and Podkladkin, Serguei and Pottgen, Ruth and Poffenberger, Paul and Poggioli, Luc and Pogrebnyak, Ivan and Pohl, David-Leon and Pokharel, Ishan and Polacek, Stanislav and Polesello, Giacomo and Poley, Anne-Luise and Polifka, Richard and Polini, Alessandro and Politis, Epaminondas and Pollard, Chris and Pollock, Zachary and Polychronakos, Venetios and Pompa Pacchi, Elena and Ponomarenko, Daniil and Ponsot, Patrick and Pontecorvo, Ludovico and Pontoriere, Giuseppe and Ponzio, Bruno and Popa, Stefan and Popeneciu, Gabriel and Porter, Ryan Douglas and Portillo Quintero, Dilia Maria and Pospisil, Stanislav and Postolache, Petronel and Potamianos, Karolos and Potrap, Igor and Potter, Tina and Potti, Harish and Poulsen, Trine and Poveda Torres, Ximo and Pozo Astigarraga, Eukeni and Prades Ibanez, Alberto and Pranav Bhagawath Prasad, Pranav Bhagawath and Prapa, Maria Myrto and Pretel, Jose and Price, Darren and Primavera, Margherita and Principe Martin, Miguel Angel and Privara, Radek and Proffitt, Mason and Proklova, Nadezhda and Prokofiev, Kirill and Prono, Gilles and Proto, Giorgia and Protopopescu, Serban and Proudfoot, James and Przybycien, Mariusz and Puddefoot, Joshua Eldon and Pudzha, Dennis and Puzo, Patrick Michel and Pyatiizbyantseva, Diana and Qian, Jianming and Qian, Weiming and Dong, Qichen and Qin, Quake and Qiu, Tong and Quadt, Arnulf and Queitsch-Maitland, Michaela and Quercia, Lorenzo and Quetant, Guillaume and Rabanal Bolanos, Gabriel and Rabel, Joseph and Rafanoharana, Dimbiniaina and Ragusa, Francesco and Rainbolt, Lacey and Raine, Johnny and Rajagopalan, Srini and Ramakoti, Ekaterina and Ramirez-Berend, Ian Alejandro and Ran, Kunlin and Rapheeha, Phuti and Rashid, Tasneem and Raskina, Valentina and Rassloff, Damir Fabrice and Rave, Stefan and Ravina, Baptiste and Ravinovich, Ilia and Raymond, Michel and Read, Alexander Lincoln and Readioff, Nathan Peter and Rebuzzi, Daniela and Redlinger, George and Reeves, Kendall and Reidelsturz, Joshua Aaron and Reikher, David and Rej, Amartya and Rembser, Christoph and Renardi, Alessia and Renda, Michele and Rendel, Marian Benedikt and Renklioglu, Ahmet and Renner, Frederic and Rennie, Adam and Resconi, Silvia and Ressegotti, Martina and Resseguie, Elodie Deborah and Rettie, Sebastien and Reyes Rivera, Jose Gabriel and Reynolds, Bryan and Reynolds, Elliot and Rezaei Estabragh, Mohsen and Rezanova, Olga and Reznicek, Pavel and Riallot, Marc and Ribaric, Neza and Ricci, Ester and Richert, William and Richter, Robert and Richter, Stefan and Richter-Was, Elzbieta and Ridel, Melissa and Ridouani, Selaiman and Rieck, Patrick and Riedler, Petra and Riegel, Christian and Rijssenbeek, Michael and Rimoldi, Adele and Rimoldi, Marco and Rinaldi, Lorenzo and Rinn, Timothy Thomas and Rinnagel, Maximilian Paul and Ripellino, Giulia and Riu, Imma and Rivadeneira Bracho, Pablo Andres and Rivera Vergara, Juan Cristobal and Rizatdinova, Flera and Rizvi, Eram Syed and Rizzi, Chiara and Roberts, Bryn Arthur and Roberts, Ryan and Robertson, Steven and Robichaud-Veronneau, Andree and Robin, Matthieu and Robinson, Dave and Robles Gajardo, Carolina Michel and Robles Manzano, Marisol and Robson, Aidan and Rocchi, Alessandro and Roda, Chiara and Rodriguez Bosca, Sergi and Rodriguez Garcia, Yohany and Rodriguez Rodriguez, Arturo and Rodriguez Vera, Ana Maria and Roe, Shaun and Roemer, Jonas Till and Roepe-Gier, Amber and Roggel, Jens and Rohne, Ole and Roich, Alexander and Rojas Caballero, Rimsky Alejandro and Roland, Benoit and Roland, Christophe Pol A. and Roloff, Jennifer and Romaniouk, Anatoli and Romano, Emanuele and Romano, Marino and Romero, Anabel and Rompotis, Nikolaos and Roos, Lydia and Rosati, Stefano and Roscilli, Lorenzo and Rose-Dulcina, Louis and Rößl, Stefan and Rosser, Benjamin John and Rossi, Cecilia and Rossi, Eleonora and Rossi, Elvira and Rossi, Fabrizio and Rossi, Leonardo and Rossini, Lorenzo and Rosten, Rachel Christine and Rotaru, Marina and Rottler, Benjamin and Rougier, Charline and Rousseau, David and Rousso, David and Rovani, Alessandro and Rovelli, Giulia and Roy, Avik and Rozanov, Alexandre and Rozen, Yoram and Ruan, Ruanxf and Rubio Jimenez, Adrian and Ruby, Adam James and Ruelas Rivera, Victor Hugo and Ruggeri, Tristan Andrew and Ruggieri, Alessandro and Ruggieri, Daniele and Ruehr, Frederik and Ruiz Martinez, Arantxa and Rummler, Andre and Rurikova, Zuzana and Rusakovich, Nikolai and Ruscino, Ettore and Russell, Heather and Rutherfoord, John P. and Růžička, Ondřej and Rybacki, Katherine Amy and Rybar, Martin and Rye, Eli Baverfjord and Ryjov, Vladimir and Ryzhov, Andrey and Sabater Iglesias, Jorge Andres and Sabatini, Fabrizio and Sabatini, Paolo and Sabetta, Luigi and Sadrozinski, Hartmut and Safai Tehrani, Francesco and Safarzadeh Samani, Batool and Safdari, Murtaza and Saha, Shreya and Sahinsoy, Merve and Saimpert, Matthias and Saito, Masahiko and Saito, Tomoyuki and Sajid, Muhammad and Salamani, Dalila and Salamanna, Giuseppe and Salnikov, Andy and Salomon, Franck and Salt, Jose and Salvador Salas, Adrian and Salvatore, Daniela and Salvatore, Fabrizio and Salzburger, Andreas and Samarati, Jerome and Sammel, Dirk and Sampsonidis, Dimos and Sampsonidou, Despoina and Sanchez Martinez, Francisco Javier and Sanchez Pineda, Arturos and Sanchez Sebastian, Victoria and Sandaker, Heidi and Sander, Christian and Sandesara, Jay Ajitbhai and Sandhoff, Marisa and Sandoval Usme, Carlos and Sankey, David and Sanny, Bernd and Sano, Takane and Sansoni, Andrea and Santi, Lorenzo and Santoni, Claudio and Santos, Helena and Santpur, Sai Neha and Santra, Arka and Saoucha, Kamal and Mendes Saraiva, Joao Gentil and Sardain, Jad Mathieu and Sasaki, Osamu and Sato, Koji and Satterthwaite, Toby and Satzkowski, Reinhard and Sauer, Christof and Sauerburger, Frank and Sauvan, Emmanuel and Sauve, Ryan Matthew and Savard, Pierre and Sawada, Ryu and Sawyer, Craig and Sawyer, Lee and Sayago Galvan, Ivan and Sbarra, Carla and Sbrizzi, Antonio and Scagliotti, Claudio and Scanlon, Timothy Paul and Schaarschmidt, Jana and Schacht, Peter and Schaefer, Douglas Michael and Schaefer, Uli and Schaffer, R.D. and Schaile, Dorothee and Schaile, Otto and Schamberger, Robert Dean, Jr. and Schanet, Eric and Scharf, Christian and Schefer, Meinrad Moritz and Chtcheguelski, Valery and Scheirich, Daniel and Schenck, Ferdinand and Scherino, Lorenzo and Schernau, Michael and Scheulen, Chris and Schiavi, Carlo and Schillaci, Zachary Michael and Schioppa, Enrico, Junior. and Schioppa, Marco and Schlag, Bastian and Schleicher, Katharina and Schlenker, Stefan and Schmeing, Jonas and Schmidt, Mustafa Andre and Schmieden, Kristof and Schmitt, Christian and Schmitt, Stefan and Schnarr, Rodney Micheal and Schoeffel, Laurent Olivier and Schoening, Andre and Scholer, Patrick and Schopf, Elisabeth and Schorlemmer, Andre Lukas and Schott, Matthias and Schovancova, Jaroslava and Schramm, Steven and Schroeder, Frederic and Schultz-Coulon, Hans-Christian and Schumacher, Jorn and Schumacher, Markus and Schumm, Bruce Andrew and Schune, Philippe and Schwartz, Hava Rhian and Schwartzman, Ariel Gustavo and Schwarz, Thomas Andrew and Schwemling, Philippe and Schwienhorst, Reinhard and Sciandra, Andrea and Sciolla, Gabriella and Sciuccati, Augusto and Scott, Garrett Joseph and Scuri, Fabrizio and Scutti, Federico and Sebastiani, Cristiano and Secord, Chris and Sedlaczek, Kevin and Seema, Pienpen and Seidel, Sally and Seiden, Abraham and Seidlitz, Blair Daniel and Seitz, Claudia and Seixas, Jose and Sekhniaidze, Givi and Sekula, Stephen Jacob and Selem, Luka and Seletskiy, Alexandre and Semprini Cesari, Nicola and Sen, Sourav and Sengupta, Debajyoti and Senthilkumar, Varsha and Serin, Laurent and Serkin, Leonid and Serochkin, Mikhail and Sessa, Marco and Severini, Horst and Sexton, Kenneth Alan and Sforza, Federico and Sfyrla, Anna and Shabalina, Elizaveta and Shafto, Gene and Shaheen, Rabia and Shahinian, Jeff and Shaked, Ohad and Shaked Renous, Dan and Shan, Lianyou and Shapiro, Marjorie and Sharma, Abhishek and Sharma, Abhishek and Sharma, Punit and Sharma, Surabhi and Shatalov, Sppavel and Shaw, Kate and Shaw, Savanna and Shen, Qiuping and Sheppard, Damian Joseph and Sherpa, Pasang Nuri and Sherwood, Peter and Shi, Liaoshan and Shimmin, Chase Owen and Shimogama, Yoshihiro and Shinner, James David and Shipsey, Ian and Shirabe, Shohei and Shiyakova, Mariya and Shlomi, Jonathan and Shoa, Meir and Shochet, Mel and Shojaii, Seyed Ruhollah and Shooltz, Dean Daniel and Shope, David Richard and Shrestha, Suyog and Shrif, Esra Mohammed and Shroff, Maheyer Jamshed and Shutov, Alexander and Sicho, Petr and Sickles, Anne Marie and Sideras Haddad, Elias and Sidiropoulou, Ourania and Sidoti, Antonio and Siegert, Frank and Sijacki, Dorde and Sikora, Rafal and Sili, Francisco and Cardoso Silva, Julia Manuela and Silva Oliveira, Marcos Vinicius and Silverstein, Samuel and Simion, Stefan and Simola, Vesa and Simoniello, Rosa and Simpson, Ethan Lewis and Simpson, Liana and Simpson, Nathan Daniel and Simsek, Sinem and Sindhu, Sreelakshmi and Sinervo, Pekka and Singh, Shuvay and Singh, Sundeep and Singh, Sahibjeet and Sinha, Supriya and Sinha, Sukanya and Sioli, Maximiliano and Sippach, William and Siral, Ismet and Sivoklokov, Serguei and Siyad, Mohamed Jimcale and Sjoelin, Joergen and Skaf, Ali and Skorda, Eleni and Skubic, Patrick and Slawinska, Magdalena and Sliwa, Krzysztof and Smakhtin, Vladimir and Smart, Ben Harry and Smiesko, Juraj and Smirnov, Serge and Smirnov, Yury and Smirnova, Lidia and Smirnova, Oxana and Smith, Andrew Caldon and Smith, Dale Shane and Smith, Emily Ann and Smith, Hayden Alexander and Smith, James and Smith, Rachel Emma Clarke and Smizanska, Maria and Smolek, Karel and Smykiewicz, Andrzej and Snesarev, Andrei and Snoek, Hella and Snyder, Scott and Sobie, Randy and Soffer, Abi and Solans Sanchez, Carlos and Soldatov, Evgeny and Soldevila Serrano, Urmila and Solis, Michelle Ann and Soliveres Riviere, Francoise Simone and Solodkov, Sanya and Solomon, Shalu and Soloshenko, Aleksey and Solovieva, Ksenia and Solovyanov, Oleg and Solovyev, Victor and Soluk, Richard and Sommer, Philip and Sonay, Anil and Song, Wen Yi and Sonneveld, Jory and Sopczak, Andre and Sopio, Alex and Sopkova, Filomena and Sorbe, Jérôme and Sothilingam, Varsiha and Sottocornola, Simone and Soualah, Rachik and Soumaimi, Zainab and South, David and Soyk, Daniel and Spagnolo, Stefania and Spalla, Margherita and Spano, Francesco and Speers, Peter and Sperlich, Dennis and Spigo, Giancarlo and Spina, Mario and Spinali, Sebastiano and Spiteri, Dwayne Patrick and Spiwoks, Ralf and Spousta, Martin and Staats, Ezekiel and Stabile, Alberto and Staley, Richard John and Stamen, Rainer and Stamenkovic, Marko and Stamoulos, Ioannis and Stampekis, Alexios and Standke, Mark and Stanecka, Ewa and Stange, Max Vincent and Stanislaus, Beojan and Stanitzki, Marcel and Stankaityte, Migle and Stapf, Birgit Sylvia and Starchenko, Jenya and Stark, Giordon Holtsberg and Stark, Jan and Starko, Darij Markian and Staroba, Pavel and Starovoitov, Pavel and Staerz, Steffen and Staszewski, Rafal and Stavropoulos, George and Steentoft, Jonas and Steinberg, Peter Alan and Steinhebel, Amanda and Stelzer, Bernd and Stelzer, Joerg and Stelzer-Chilton, Oliver and Stenzel, Hasko and Stevenson, Thomas James and Stewart, Graeme A. and Stockton, Mark and Stoicea, Gabriel and Stolarski, Marcin and Stonjek, Stefan and Stouras, Nikos and Straessner, Arno and Strandberg, Jonas and Strandberg, Sara and Strauss, Mike and Strebler, Thomas and Strickland, Vance and Strizenec, Pavol and Strohmer, Raimund and Strom, David and Strom, Lars Rickard and Stroynowski, Ryszard and Strubig, Antonia and Stucci, Stefania Antonia and Stugu, Bjarne and Stupak, John and Sturdy, Jared and Styles, Nicholas and Su, Dong and Su, Shixiang and Su, Wanyun and Su, Xiaowen and Sugizaki, Kaito and Sulin, Vladimir and Sullivan, Matthew James and Sultan, D.M.S. and Sultanaliyeva, Laily and Sultanov, Salekh and Sumida, Toshi and Sun, Quan and Sun, Siyuan and Sun, Shaojun and Gudnadottir, Olga and Sutton, Mark and Svatos, Michal and Swiatlowski, Maximilian J. and Swirski, Thorben and Sykora, Ivan and Sykora, Martin and Sykora, Tomas and Ta, Duc Bao and Tackmann, Kerstin and Taffard, Anyes and Tafirout, Reda and Tafoya Vargas, Juan Salvador and Taghavirad, Saeed and Taibah, Reem Hani M. and Takashima, Ryuichi and Takeda, Kosuke and Takeva, Emily Petrova and Takubo, Yosuke and Talby, Mossadek and Talyshev, Alexei and Tam, Kai Chung and Tamir, Nadav Michael and Tanaka, Aoto and Tanaka, Junichi and Tanaka, Rei and Tanasini, Martino and Tang, Jiannan and Tang, Shaochun and Tao, Zhengcheng and Tapia Araya, Sebastian and Tapprogge, Stefan and Tar, Bora and Tarek, Ahmed and Tarem, Shlomit and Tarem, Zvi and Tariq, Khuram and Tarna, Grigore and Tartarelli, Francesco and Tas, Petr and Tasevsky, Marek and Tasevsky, Mito and Tassi, Enrico and Tate, Aric and Tateno, Gen and Tayalati, Yahya and Taylor, Geoffrey Norman and Taylor, Wendy Jane and Teagle, Hamish Edward and Tee, Amy and Teixeira De Lima, Rafael and Teixeira-Dias, Pedro and Teoh, Jia Jian and Terashi, Koji and Terron Cuadrado, Juan and Terzo, Stefano and Testa, Marianna and Teterin, Peter and Teurnier, Marie-Solene and Teuscher, Richard and Thaler, Alexander and Theiner, Ondrej and Themistokleous, Neofytos and Theveneaux-Pelzer, Timothee and Thielmann, Oliver and Thomas, Chris and Thomas, David William and Thomas, James Oscar and Thomas, Juergen and Thompson, Emily Anne and Thompson, Paul and Thomson, Evelyn Jean and Thorpe, Edward James and Tian, Yusong and Tikhomirov, Vladimir and Tikhonov, Iouri and Timoshenko, Sergei and Ting, Edmund Xiang Lin and Tipton, Paul Louis and Tisserant, Sylvain and Tlou, Humphry and Tnourji, Abdellah and Tobias, Juergen and Todome, Kazuki and Todorov, Teddy and Todorova, Sarka and Todt, Stefanie and Togawa, Manabu and Tojo, Junji and Tokar, Stano and Tokushuku, Katsuo and Toldaiev, Alex and Tombs, Rupert and Tomoto, Makoto and Tompkins, Daniel and Tompkins, Lauren Alexandra and Topolnicki, Kacper Wojciech and Tornambe, Peter and Torrence, Eric and Torres, Heberth and Torro Pastor, Emma and Toscani, Mariana and Tosciri, Cecilia and Tost, Marc and Tovey, Daniel and Traeet, Are Sivertsen and Tranchand, Laure and Trandafir, Iulia-Stefania and Trantou, Foteini and Trattino, Pietro and Travaglini, Riccardo and Trefzger, Thomas and Tricoli, Alessandro and Trigger, Isabel and Trincaz-Duvoid, Sophie and Trischuk, Dominique Anderson and Trocme, Benjamin and Troeglazov, Ivan and Trofymov, Artur and Troncon, Clara and Troska, Georg and Trotta, Danilo and Trovatelli, Monica and Trovato, Marco and Truong, Thi Ngoc Loan and Trzebinski, Maciej and Trzupek, Adam and Tsai, Fang-Ying and Tsai, Meng-Ju and Tse, Wan Ho and Tsiafis, Yoannis and Tsiamis, Angelos and Tsiareshka, Pavel and Tsigaridas, Stergios and Tsirigotis, Apostolos and Tsiskaridze, Vakhtang and Tskhadadze, Edisher and Tsopoulou, Maria-Evanthia and Tsujikawa, Yoshiaki and Tsukerman, Ilia and Tsulaia, Vakhtang and Tsuno, Soshi and Tsur, Omer and Tsybyshev, Dmitry and Tu, Yanjun and Tudorache, Alexandra and Tudorache, Valentina and Tuna, Alexander Naip and Turchikhin, Semen and Turco, Paola and Turk Cakir, Ilkay and Turra, Ruggero and Turtuvshin, Tulgaa and Tusi, Enrico and Tuts, Mike and Tzamarias, Spyros and Tzanis, Polyneikis and Tzanos, Stamatios and Tzovara, Eftychia and Uchida, Kenta and Ukah, Kelechi Rock and Ukegawa, Fumihiko and Ulloa Poblete, Pablo Augusto and Umaka, Ejiro Naomi and Unal, Guillaume and Unal, Mesut and Undrus, Alexander and Unel, Gokhan and Uno, Kenta and Urban, Josef and Urbasek, Vladimir and Urquijo, Phillip and Usai, Giulio and Ushioda, Risa and Usman, Muhammad and Usseglio, Michel and Uysal, Zekeriya and Vacavant, Laurent and Vacek, Vic and Vacher, Thierry and Vachon, Brigitte and Vadla, Knut Oddvar Hoie and Vafeiadis, Theodoros and Vaitkus, Andrius and Valderanis, Chrysostomos and Valdes Santurio, Eduardo and Valente, Marco and Valentinetti, Sara and Valero Biot, Alberto and Vallier, Alexis and Valls Ferrer, Juan and Van Arneman, Dylan Remberto and Van Daalen, Tal Roelof and Van Gemmeren, Peter and Van Overbeek, Martijn and Van Rijnbach, Milou and Van Stroud, Samuel and Van Vulpen, Ivo and Vanadia, Marco and Vandelli, Wainer and Vandenbroucke, Maxence and Van De Wall, Evan Richard and Vannicola, Damiano and Vannoli, Leonardo and Varga-Rehling, Attila and Vari, Riccardo and Varnes, Erich Ward and Varni, Carlo and Mete, Tulin and Varouchas, Dimitris and Varriale, Lorenzo and Varvell, Kevin and Vasile, Matei and Vaslin, Louis and Vasquez, Gerardo and Vazeille, Francois and Vazquez Schroeder, Tamara and Vdovin, Aleksander and Veatch, Jason Robert and Vecchio, Valentina and Veen, Michiel Jan and Veliscek, Iza and Veloce, Laurelle Maria and Veloso, Filipe and Veneziano, Stefano and Ventura, Andrea and Venturi, Nicola and Verbytskyi, Andrii and Vercellati, Filippo and Verducci, Monica and Vergain, Maurice and Vergis, Christos and Verissimo De Araujo, Micael and Verkerke, Wouter and Verlaat, Bart and Vermeulen, Jos and Vernieri, Caterina and Verschuuren, Pim Jordi and Vessella, Makayla and Vetterli, Michel Joseph and Vgenopoulos, Andreas and Viaux Maira, Nicolas and Vichoudis, Paschalis and Vickey, Trevor and Vickey Boeriu, Oana and Viehhauser, Georg and Vieira De Souza, Julio and Vigani, Luigi and Vigeolas, Eric and Villa, Mauro and Villaplana, Miguel and Villhauer, Elena Michelle and Vilucchi, Elisabetta and Vincter, Manuella and Vinogradov, Mikhail and Virdee, Govindraj Singh and Vishwakarma, Akanksha and Vittori, Camilla and Vivarelli, Iacopo and Vlachos, Sotiris and Vladimirov, Vangelis and Voevodina, Elena and Vogel, Fabian and Vogt, Sven and Vokac, Petr and Von Ahnen, Janik and Von Torne, Eckhard and Vormwald, Benedikt and Vorobel, Vit and Vorobev, Konstantin and Vos, Marcel and Voss, Katharina and Vossebeld, Joost and Vozak, Matous and Vozdecky, Lubos and Vranjes, Nenad and Vranjes Milosavljevic, Marija and Vreeswijk, Marcel and Vuillemin, Cyrille and Vuillermet, Raphael and Vujinovic, Olivera and Vukotic, Ilija and Wada, Sayaka and Wagner, Cooper and Wagner, Wolfgang and Wahdan, Shayma and Wahlberg, Hernan Pablo and Wakasa, Rena and Wakida, Moe and Walbrecht, Verena Maria and Walder, James William and Walker, Rodney and Walker, Robert Bond and Walkowiak, Wolfgang and Wang, Ann Miao and Wang, Alex Zeng and Wang, Chen and Wang, Chenliang and Wang, Haichen and Wang, Jiawei and Wang, Jinglu and Wang, Jinhong and Wang, Qiang and Wang, Renjie and Wang, Rongkun and Wang, Rui and Wang, Song-Ming and Wang, Shuanggeng and Wang, Tao and Wang, Weitao and Wang, Xu and Wang, Xin and Wang, Xinxin and Wang, Xiaoning and Wang, Xi and Wang, Yufeng and Wang, Yuhao and Wang, Zirui and Wang, Zhen and Wang, Zhichen and Warburton, Andreas and Ward, Robert James and Warrack, Neil and Watson, Alan and Watson, Harriet and Watson, Miriam and Watts, Gordon and Waugh, Benedict Martin and Weaverdyck, Curtis John and Webb, Aaron and Weber, Christian and Weber, Hannsjorg and Weber, Jens and Weber, Maarten and Weber, Michele and Weber, Stephen and Weber, Sebastian Mario and Wei, Chuanshun and Wei, Yingjie and Weidberg, Anthony and Weingarten, Jens and Weirich, Marcel and Weiser, Christian and Welch, Steven and Wells, Craig John and Wells, Pippa and Wenaus, Torre and Wendland, Bjoern and Wengler, Thorsten and Wenke, Nina Stephanie and Wensing, Marius and Wermes, Norbert and Wessels, Martin and Whalen, Kate and Wharton, Andrew Mark and White, Aaron Stephen and White, Andrew and White, Martin John and Whiteson, Daniel and Wickremasinghe, Lakmin and Wiedenmann, Werner and Wiel, Christian and Wielers, Monika and Wiglesworth, Craig and Wiik-Fuchs, Liv and Wilbern, Daniel John and Wilkens, Henric and Williams, Daniel and Williams, Hugh and Williams, Sarah Louise and Willocq, Stephane and Windischhofer, Philipp and Wingerter, Isabelle and Winklmeier, Frank and Winter, Benedict Tobias and Winter, Joshua Krystian and Wittgen, Matthias and Wittig, Tobias and Wobisch, Markus and Woelker, Ricardo and Wollrath, Julian and Wolniewicz, Kevin and Wolter, Marcin and Wolters, Helmut and Wong, Vincent Wai Sum and Wongel, Alicia and Worm, Steven and Wosiek, Barbara Krystyna and Wotschack, Joerg and Woyshville, Aaron and Wozniak, Krzysztof Wieslaw and Wraight, Kenneth Gibb and Wu, Jinfei and Wu, Minlin and Wu, Mengqing and Wu, Sau Lan and Wu, Weihao and Wu, Wenjing and Wu, Xin and Wu, Yusheng and Wu, Zhibo and Wurzinger, Jonas and Wyatt, Terry and Wynne, Benjamin Michael and Xella, Stefania and Xia, Ligang and Xia, Mingming and Xiang, Jianhuan and Xiao, Xiong and Xie, Mingzhe and Xie, Xiangyu and Xin, Shuiting and Xiong, Junwen and Xiotidis, Ioannis and Xu, Da and Xu, Hanlin and Xu, Hao and Xu, Hao and Xu, Lailin and Xu, Riley and Xu, Rui and Xu, Tairan and Xu, Wenhao and Xu, Yue and Xu, Zhongyukun and Xu, Zijun and Yabsley, Bruce Donald and Yacoob, Sahal and Yamaguchi, Naoki and Yamaguchi, Yohei and Yamamoto, Shimpei and Yamauchi, Hiroki and Yamazaki, Tomohiro and Yamazaki, Yuji and Yan, Jun and Yan, Siyuan and Yan, Zhen and Yandyan, Armen and Yang, Haijun and Yang, Hongtao and Yang, Siqi and Yang, Tianyi and Yang, Xiao and Yang, Xuan and Yang, Yi-Lin and Yang, Zhe and Yao, Lin and Yao, Wei-Ming and Yap, Yee Chinn and Ye, Hanfei and Ye, Hua and Ye, Jingbo and Ye, Shuwei and Ye, Xinmeng and Yeh, Yoran and Yeletskikh, Ivan and Yeo, Beom Ki and Yexley, Melissa and Yildiz, Cenk and Yin, Pengqi and Yin, Weigang and Yorita, Kohei and Younas, Sulman and Young, Christopher and Young, Charlie and Yu, Yi and Yuan, Man and Yuan, Rui and Yue, Luzhan and Yue, Xiaoguang and Yukhimchuk, Sergey and Zaazoua, Mohamed and Zabinski, Bartlomiej Henryk and Zachariadou, Katerina and Zaghia, Hamid and Zahradnik, Vit and Zaid, Estifa'A and Zakareishvili, Tamar and Zakharchuk, Nataliia and Zambito, Stefano and Zamora Saa, Jilberto Antonio and Zang, Jiaqi and Zanzi, Daniele and Zaplatilek, Ota and Zeissner, Sonja Verena and Zeitnitz, Christian and Zeng, Jiancong and Zenger, Todd and Zenin, Oleg and Zenis, Tibor and Zenz, Seth and Zerradi, Soufiane and Zerwas, Dirk and Zhai, Mingjie and Zhang, Bowen and Zhang, Dengfeng and Zhang, Jie and Zhang, Jinlong and Zhang, Kaili and Zhang, Lei and Zhang, Peng and Zhang, Rui and Zhang, Shuzhou and Zhang, Tingyu and Zhang, Xiangke and Zhang, Xueyao and Zhang, Yulei and Zhang, Zhicai and Zhang, Zhiqing Philippe and Zhao, Haoran and Zhao, Pingchuan and Zhao, Tongbin and Zhao, Xiandong and Zhao, Yuzhan and Zhao, Zhengguo and Zhemchugov, Alexey and Zheng, Xiangxuan and Zheng, Zhi and Zhivun, Elena and Zhong, Dewen and Zhou, Bing and Zhou, Chen and Zhou, Hao and Zhou, Ning and Zhou, Shun and Zhou, You and Zhu, Chengguang and Zhu, Chenzheng and Zhu, Heling and Zhu, Hongbo and Zhu, Junjie and Zhu, Yifan and Zhu, Yingchun and Zhuang, Xuai and Zhukov, Konstantin and Zhulanov, Vladimir and Zibell, Andre and Zich, Jan and Zimine, Nikolai and Zimmermann, Jorg and Zimmermann, Stephanie Ulrike and Zinsser, Joachim and Ziolkowski, Michal and Zivkovic, Lidija and Zoccoli, Antonio and Zoch, Knut and Zolkin, Igor and Zonca, Eric and Zorbas, Theodore and Zormpa, Olga and Zou, Wenkai and Zuk, George and Zullo, Antonio and Zwalinski, Lukasz},
	year = {2023},
	note = {\_eprint: 2305.16623},
	annote = {233 pages in total, author list starting page 214, 116 figures, 15 tables, submitted to JINST. All figures including auxiliary figures are available at http://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/GENR-2019-02/},
	file = {Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\8WQ5C5EQ\\Aad et al. - 2023 - The ATLAS Experiment at the CERN Large Hadron Coll.pdf:application/pdf},
}

@article{atkin_review_2015,
	title = {Review of jet reconstruction algorithms},
	volume = {645},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/645/1/012008},
	doi = {10.1088/1742-6596/645/1/012008},
	abstract = {Accurate jet reconstruction is necessary for understanding the link between the unobserved partons and the jets of observed collimated colourless particles the partons hadronise into. Understanding this link sheds light on the properties of these partons. A review of various common jet algorithms is presented, namely the Kt, Anti-Kt, Cambridge/Aachen, Iterative cones and the SIScone, highlighting their strengths and weaknesses. If one is interested in studying jets, the Anti-Kt algorithm is the best choice, however if ones interest is in the jet substructures then the Cambridge/Aachen algorithm would be the best option.},
	language = {en},
	urldate = {2024-01-15},
	journal = {Journal of Physics: Conference Series},
	author = {Atkin, Ryan},
	month = oct,
	year = {2015},
	pages = {012008},
	file = {Atkin - 2015 - Review of jet reconstruction algorithms.pdf:C\:\\Users\\David\\Zotero\\storage\\EH4RAF9T\\Atkin - 2015 - Review of jet reconstruction algorithms.pdf:application/pdf},
}

@article{cacciari_anti-k_t_2008,
	title = {The anti-k\_t jet clustering algorithm},
	volume = {2008},
	issn = {1029-8479},
	url = {http://arxiv.org/abs/0802.1189},
	doi = {10.1088/1126-6708/2008/04/063},
	abstract = {The k\_t and Cambridge/Aachen inclusive jet finding algorithms for hadron-hadron collisions can be seen as belonging to a broader class of sequential recombination jet algorithms, parametrised by the power of the energy scale in the distance measure. We examine some properties of a new member of this class, for which the power is negative. This ``anti-k\_t'' algorithm essentially behaves like an idealised cone algorithm, in that jets with only soft fragmentation are conical, active and passive areas are equal, the area anomalous dimensions are zero, the non-global logarithms are those of a rigid boundary and the Milan factor is universal. None of these properties hold for existing sequential recombination algorithms, nor for cone algorithms with split--merge steps, such as SISCone. They are however the identifying characteristics of the collinear unsafe plain ``iterative cone'' algorithm, for which the anti-k\_t algorithm provides a natural, fast, infrared and collinear safe replacement.},
	number = {04},
	urldate = {2024-01-15},
	journal = {Journal of High Energy Physics},
	author = {Cacciari, Matteo and Salam, Gavin P. and Soyez, Gregory},
	month = apr,
	year = {2008},
	note = {arXiv:0802.1189 [hep-ph]},
	keywords = {High Energy Physics - Phenomenology},
	pages = {063--063},
	annote = {Comment: 12 pages, 5 figures. Small changes made for publication. Version published in JHEP},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\3RZKA65R\\0802.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\VWFNIXUB\\Cacciari et al. - 2008 - The anti-k_t jet clustering algorithm.pdf:application/pdf},
}

@article{albertsson_machine_2019,
	title = {Machine {Learning} in {High} {Energy} {Physics} {Community} {White} {Paper}},
	url = {http://arxiv.org/abs/1807.02876},
	abstract = {Machine learning has been applied to several problems in particle physics research, beginning with applications to high-level physics analysis in the 1990s and 2000s, followed by an explosion of applications in particle and event identification and reconstruction in the 2010s. In this document we discuss promising future research and development areas for machine learning in particle physics. We detail a roadmap for their implementation, software and hardware resource requirements, collaborative initiatives with the data science community, academia and industry, and training the particle physics community in data science. The main objective of the document is to connect and motivate these areas of research and development with the physics drivers of the High-Luminosity Large Hadron Collider and future neutrino experiments and identify the resource needs for their implementation. Additionally we identify areas where collaboration with external communities will be of great benefit.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Albertsson, Kim and Altoe, Piero and Anderson, Dustin and Anderson, John and Andrews, Michael and Espinosa, Juan Pedro Araque and Aurisano, Adam and Basara, Laurent and Bevan, Adrian and Bhimji, Wahid and Bonacorsi, Daniele and Burkle, Bjorn and Calafiura, Paolo and Campanelli, Mario and Capps, Louis and Carminati, Federico and Carrazza, Stefano and Chen, Yi-fan and Childers, Taylor and Coadou, Yann and Coniavitis, Elias and Cranmer, Kyle and David, Claire and Davis, Douglas and De Simone, Andrea and Duarte, Javier and Erdmann, Martin and Eschle, Jonas and Farbin, Amir and Feickert, Matthew and Castro, Nuno Filipe and Fitzpatrick, Conor and Floris, Michele and Forti, Alessandra and Garra-Tico, Jordi and Gemmler, Jochen and Girone, Maria and Glaysher, Paul and Gleyzer, Sergei and Gligorov, Vladimir and Golling, Tobias and Graw, Jonas and Gray, Lindsey and Greenwood, Dick and Hacker, Thomas and Harvey, John and Hegner, Benedikt and Heinrich, Lukas and Heintz, Ulrich and Hooberman, Ben and Junggeburth, Johannes and Kagan, Michael and Kane, Meghan and Kanishchev, Konstantin and Karpiński, Przemysław and Kassabov, Zahari and Kaul, Gaurav and Kcira, Dorian and Keck, Thomas and Klimentov, Alexei and Kowalkowski, Jim and Kreczko, Luke and Kurepin, Alexander and Kutschke, Rob and Kuznetsov, Valentin and Köhler, Nicolas and Lakomov, Igor and Lannon, Kevin and Lassnig, Mario and Limosani, Antonio and Louppe, Gilles and Mangu, Aashrita and Mato, Pere and Meenakshi, Narain and Meinhard, Helge and Menasce, Dario and Moneta, Lorenzo and Moortgat, Seth and Neubauer, Mark and Newman, Harvey and Otten, Sydney and Pabst, Hans and Paganini, Michela and Paulini, Manfred and Perdue, Gabriel and Perez, Uzziel and Picazio, Attilio and Pivarski, Jim and Prosper, Harrison and Psihas, Fernanda and Radovic, Alexander and Reece, Ryan and Rinkevicius, Aurelius and Rodrigues, Eduardo and Rorie, Jamal and Rousseau, David and Sauers, Aaron and Schramm, Steven and Schwartzman, Ariel and Severini, Horst and Seyfert, Paul and Siroky, Filip and Skazytkin, Konstantin and Sokoloff, Mike and Stewart, Graeme and Stienen, Bob and Stockdale, Ian and Strong, Giles and Sun, Wei and Thais, Savannah and Tomko, Karen and Upfal, Eli and Usai, Emanuele and Ustyuzhanin, Andrey and Vala, Martin and Vasel, Justin and Vallecorsa, Sofia and Verzetti, Mauro and Vilasís-Cardona, Xavier and Vlimant, Jean-Roch and Vukotic, Ilija and Wang, Sean-Jiun and Watts, Gordon and Williams, Michael and Wu, Wenjing and Wunsch, Stefan and Yang, Kun and Zapata, Omar},
	month = may,
	year = {2019},
	note = {arXiv:1807.02876 [hep-ex, physics:physics, stat]},
	keywords = {Computer Science - Machine Learning, High Energy Physics - Experiment, Physics - Computational Physics, Statistics - Machine Learning},
	annote = {Comment: Editors: Sergei Gleyzer, Paul Seyfert and Steven Schramm},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\62SUQ9A7\\1807.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\A6NCG4UU\\Albertsson et al. - 2019 - Machine Learning in High Energy Physics Community .pdf:application/pdf},
}

@article{kasieczka_machine_2019,
	title = {The {Machine} {Learning} {Landscape} of {Top} {Taggers}},
	volume = {7},
	issn = {2542-4653},
	url = {http://arxiv.org/abs/1902.09914},
	doi = {10.21468/SciPostPhys.7.1.014},
	abstract = {Based on the established task of identifying boosted, hadronically decaying top quarks, we compare a wide range of modern machine learning approaches. Unlike most established methods they rely on low-level input, for instance calorimeter output. While their network architectures are vastly different, their performance is comparatively similar. In general, we find that these new approaches are extremely powerful and great fun.},
	number = {1},
	urldate = {2024-01-15},
	journal = {SciPost Physics},
	author = {Kasieczka, G. and Plehn, T. and Butter, A. and Cranmer, K. and Debnath, D. and Dillon, B. M. and Fairbairn, M. and Faroughy, D. A. and Fedorko, W. and Gay, C. and Gouskos, L. and Kamenik, J. F. and Komiske, P. T. and Leiss, S. and Lister, A. and Macaluso, S. and Metodiev, E. M. and Moore, L. and Nachman, B. and Nordstrom, K. and Pearkes, J. and Qu, H. and Rath, Y. and Rieger, M. and Shih, D. and Thompson, J. M. and Varma, S.},
	month = jul,
	year = {2019},
	note = {arXiv:1902.09914 [hep-ph]},
	keywords = {High Energy Physics - Phenomenology},
	pages = {014},
	annote = {Comment: Yet another tagger included!},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\49CG4V7W\\1902.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\47AA27BX\\Kasieczka et al. - 2019 - The Machine Learning Landscape of Top Taggers.pdf:application/pdf},
}

@article{duarte_fast_2018,
	title = {Fast inference of deep neural networks in {FPGAs} for particle physics},
	volume = {13},
	issn = {1748-0221},
	url = {http://arxiv.org/abs/1804.06913},
	doi = {10.1088/1748-0221/13/07/P07027},
	abstract = {Recent results at the Large Hadron Collider (LHC) have pointed to enhanced physics capabilities through the improvement of the real-time event processing techniques. Machine learning methods are ubiquitous and have proven to be very powerful in LHC physics, and particle physics as a whole. However, exploration of the use of such techniques in low-latency, low-power FPGA hardware has only just begun. FPGA-based trigger and data acquisition (DAQ) systems have extremely low, sub-microsecond latency requirements that are unique to particle physics. We present a case study for neural network inference in FPGAs focusing on a classifier for jet substructure which would enable, among many other physics scenarios, searches for new dark sector particles and novel measurements of the Higgs boson. While we focus on a specific example, the lessons are far-reaching. We develop a package based on High-Level Synthesis (HLS) called hls4ml to build machine learning models in FPGAs. The use of HLS increases accessibility across a broad user community and allows for a drastic decrease in firmware development time. We map out FPGA resource usage and latency versus neural network hyperparameters to identify the problems in particle physics that would benefit from performing neural network inference with FPGAs. For our example jet substructure model, we fit well within the available resources of modern FPGAs with a latency on the scale of 100 ns.},
	number = {07},
	urldate = {2024-01-15},
	journal = {Journal of Instrumentation},
	author = {Duarte, Javier and Han, Song and Harris, Philip and Jindariani, Sergo and Kreinar, Edward and Kreis, Benjamin and Ngadiuba, Jennifer and Pierini, Maurizio and Rivera, Ryan and Tran, Nhan and Wu, Zhenbin},
	month = jul,
	year = {2018},
	note = {arXiv:1804.06913 [hep-ex, physics:physics, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, High Energy Physics - Experiment, Physics - Instrumentation and Detectors, Statistics - Machine Learning},
	pages = {P07027--P07027},
	annote = {Comment: 22 pages, 17 figures, 2 tables, JINST revision},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\G7MMUCL5\\1804.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\4PUSN9SR\\Duarte et al. - 2018 - Fast inference of deep neural networks in FPGAs fo.pdf:application/pdf},
}

@article{aarrestad_fast_2021,
	title = {Fast convolutional neural networks on {FPGAs} with hls4ml},
	volume = {2},
	issn = {2632-2153},
	url = {http://arxiv.org/abs/2101.05108},
	doi = {10.1088/2632-2153/ac0ea1},
	abstract = {We introduce an automated tool for deploying ultra low-latency, low-power deep neural networks with convolutional layers on FPGAs. By extending the hls4ml library, we demonstrate an inference latency of \$5{\textbackslash},{\textbackslash}mu\$s using convolutional architectures, targeting microsecond latency applications like those at the CERN Large Hadron Collider. Considering benchmark models trained on the Street View House Numbers Dataset, we demonstrate various methods for model compression in order to fit the computational constraints of a typical FPGA device used in trigger and data acquisition systems of particle detectors. In particular, we discuss pruning and quantization-aware training, and demonstrate how resource utilization can be significantly reduced with little to no loss in model accuracy. We show that the FPGA critical resource consumption can be reduced by 97\% with zero loss in model accuracy, and by 99\% when tolerating a 6\% accuracy degradation.},
	number = {4},
	urldate = {2024-01-15},
	journal = {Machine Learning: Science and Technology},
	author = {Aarrestad, Thea and Loncar, Vladimir and Ghielmetti, Nicolò and Pierini, Maurizio and Summers, Sioni and Ngadiuba, Jennifer and Petersson, Christoffer and Linander, Hampus and Iiyama, Yutaro and Di Guglielmo, Giuseppe and Duarte, Javier and Harris, Philip and Rankin, Dylan and Jindariani, Sergo and Pedro, Kevin and Tran, Nhan and Liu, Mia and Kreinar, Edward and Wu, Zhenbin and Hoang, Duc},
	month = dec,
	year = {2021},
	note = {arXiv:2101.05108 [hep-ex, physics:physics, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, High Energy Physics - Experiment, Physics - Instrumentation and Detectors, Statistics - Machine Learning},
	pages = {045015},
	annote = {Comment: 18 pages, 18 figures, 4 tables},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\8X5GG3SG\\2101.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\NUTV3HJS\\Aarrestad et al. - 2021 - Fast convolutional neural networks on FPGAs with h.pdf:application/pdf},
}

@article{ngadiuba_compressing_2020,
	title = {Compressing deep neural networks on {FPGAs} to binary and ternary precision with hls4ml},
	volume = {2},
	issn = {2632-2153},
	url = {https://dx.doi.org/10.1088/2632-2153/aba042},
	doi = {10.1088/2632-2153/aba042},
	abstract = {We present the implementation of binary and ternary neural networks in the hls4ml library, designed to automatically convert deep neural network models to digital circuits with field-programmable gate arrays (FPGA) firmware. Starting from benchmark models trained with floating point precision, we investigate different strategies to reduce the network’s resource consumption by reducing the numerical precision of the network parameters to binary or ternary. We discuss the trade-off between model accuracy and resource consumption. In addition, we show how to balance between latency and accuracy by retaining full precision on a selected subset of network components. As an example, we consider two multiclass classification tasks: handwritten digit recognition with the MNIST data set and jet identification with simulated proton-proton collisions at the CERN Large Hadron Collider. The binary and ternary implementation has similar performance to the higher precision implementation while using drastically fewer FPGA resources.},
	language = {en},
	number = {1},
	urldate = {2024-01-15},
	journal = {Machine Learning: Science and Technology},
	author = {Ngadiuba, Jennifer and Loncar, Vladimir and Pierini, Maurizio and Summers, Sioni and Guglielmo, Giuseppe Di and Duarte, Javier and Harris, Philip and Rankin, Dylan and Jindariani, Sergo and Liu, Mia and Pedro, Kevin and Tran, Nhan and Kreinar, Edward and Sagear, Sheila and Wu, Zhenbin and Hoang, Duc},
	month = dec,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {015001},
	file = {IOP Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\3GPFFXLZ\\Ngadiuba et al. - 2020 - Compressing deep neural networks on FPGAs to binar.pdf:application/pdf},
}

@article{stoye_deep_2018,
	title = {Deep learning in jet reconstruction at {CMS}},
	volume = {1085},
	issn = {1742-6596},
	url = {https://dx.doi.org/10.1088/1742-6596/1085/4/042029},
	doi = {10.1088/1742-6596/1085/4/042029},
	abstract = {Deep learning has led to several breakthroughs outside the field of high energy physics, yet in jet reconstruction for the CMS experiment at the CERN LHC it has not been used so far. This report shows results of applying deep learning strategies to jet reconstruction at the stage of identifying the original parton association of the jet (jet tagging), which is crucial for physics analyses at the LHC experiments. We introduce a custom deep neural network architecture for jet tagging. We compare the performance of this novel method with the other established approaches at CMS and show that the proposed strategy provides a significant improvement. The strategy provides the first multi-class classifier, instead of the few binary classifiers that previously were used, and thus yields more information and in a more convenient way. The performance results obtained with simulation imply a significant improvement for a large number of important physics analysis at the CMS experiment.},
	language = {en},
	number = {4},
	urldate = {2024-01-15},
	journal = {Journal of Physics: Conference Series},
	author = {Stoye, Markus},
	month = sep,
	year = {2018},
	note = {Publisher: IOP Publishing},
	pages = {042029},
	file = {IOP Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\JZKHHUKK\\Stoye and collaboration - 2018 - Deep learning in jet reconstruction at CMS.pdf:application/pdf},
}

@inproceedings{wang_overview_2019,
	address = {Zhangjiajie, China},
	title = {An {Overview} of {FPGA} {Based} {Deep} {Learning} {Accelerators}: {Challenges} and {Opportunities}},
	isbn = {978-1-72812-058-4},
	shorttitle = {An {Overview} of {FPGA} {Based} {Deep} {Learning} {Accelerators}},
	url = {https://ieeexplore.ieee.org/document/8855594/},
	doi = {10.1109/HPCC/SmartCity/DSS.2019.00229},
	urldate = {2024-01-15},
	booktitle = {2019 {IEEE} 21st {International} {Conference} on {High} {Performance} {Computing} and {Communications}; {IEEE} 17th {International} {Conference} on {Smart} {City}; {IEEE} 5th {International} {Conference} on {Data} {Science} and {Systems} ({HPCC}/{SmartCity}/{DSS})},
	publisher = {IEEE},
	author = {Wang, Teng and Wang, Chao and Zhou, Xuehai and Chen, Huaping},
	month = aug,
	year = {2019},
	pages = {1674--1681},
}

@misc{noauthor_computer_nodate,
	title = {Computer {Generated} image of the {ATLAS} calorimeter},
	url = {https://cds.cern.ch/images/CERN-GE-0803015-01},
	urldate = {2024-01-15},
	file = {Computer Generated image of the ATLAS calorimeter:C\:\\Users\\David\\Zotero\\storage\\4IN2EQ4Y\\CERN-GE-0803015-01.html:text/html},
}

@article{noauthor_atlas_1996,
	title = {{ATLAS} liquid-argon calorimeter: {Technical} {Design} {Report}},
	shorttitle = {{ATLAS} liquid-argon calorimeter},
	url = {https://cds.cern.ch/record/331061},
	urldate = {2024-01-15},
	publisher = {CERN},
	year = {1996},
	doi = {10.17181/CERN.FWRW.FOOQ},
	note = {Place: Geneva
Series: Technical design report. ATLAS},
	file = {Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\SSSIGZJS\\1996 - ATLAS liquid-argon calorimeter Technical Design R.pdf:application/pdf},
}

@inproceedings{gross_frequentist_2011,
	address = {Telecon (CERN)},
	title = {Frequentist {Limit} {Recommendation}},
	url = {https://indico.cern.ch/event/126652/contributions/1343592/attachments/80222/115004/Frequentist_Limit_Recommendation.pdf},
	author = {Gross, Eilam},
	month = feb,
	year = {2011},
}

@misc{fastml_fastmachinelearninghls4ml_2023,
	title = {fastmachinelearning/hls4ml},
	url = {https://github.com/fastmachinelearning/hls4ml},
	author = {FastML, Team},
	year = {2023},
    doi = {10.5281/zenodo.1201549},
}

@article{ghielmetti_real-time_2022,
	title = {Real-time semantic segmentation on {FPGAs} for autonomous vehicles with hls4ml},
	volume = {3},
	issn = {2632-2153},
	url = {https://dx.doi.org/10.1088/2632-2153/ac9cb5},
	doi = {10.1088/2632-2153/ac9cb5},
	abstract = {In this paper, we investigate how field programmable gate arrays can serve as hardware accelerators for real-time semantic segmentation tasks relevant for autonomous driving. Considering compressed versions of the ENet convolutional neural network architecture, we demonstrate a fully-on-chip deployment with a latency of 4.9 ms per image, using less than 30\% of the available resources on a Xilinx ZCU102 evaluation board. The latency is reduced to 3 ms per image when increasing the batch size to ten, corresponding to the use case where the autonomous vehicle receives inputs from multiple cameras simultaneously. We show, through aggressive filter reduction and heterogeneous quantization-aware training, and an optimized implementation of convolutional layers, that the power consumption and resource utilization can be significantly reduced while maintaining accuracy on the Cityscapes dataset.},
	language = {en},
	number = {4},
	urldate = {2024-01-15},
	journal = {Machine Learning: Science and Technology},
	author = {Ghielmetti, Nicolò and Loncar, Vladimir and Pierini, Maurizio and Roed, Marcel and Summers, Sioni and Aarrestad, Thea and Petersson, Christoffer and Linander, Hampus and Ngadiuba, Jennifer and Lin, Kelvin and Harris, Philip},
	month = nov,
	year = {2022},
	note = {Publisher: IOP Publishing},
	pages = {045011},
	file = {IOP Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\43YDGKYW\\Ghielmetti et al. - 2022 - Real-time semantic segmentation on FPGAs for auton.pdf:application/pdf},
}

@misc{noauthor_tme1532103642766image_nodate,
	title = {tme1532103642766.image • {Viewer} • {AMD} {Adaptive} {Computing} {Documentation} {Portal}},
	url = {https://docs.xilinx.com/viewer/attachment/IVWdCfzoCeEODRZo8dBk6A/51KceOwRueyqsbOy_VA7Sg},
	urldate = {2024-01-15},
	file = {tme1532103642766.image • Viewer • AMD Adaptive Computing Documentation Portal:C\:\\Users\\David\\Zotero\\storage\\8PM6YTM5\\51KceOwRueyqsbOy_VA7Sg.html:text/html},
}

@misc{noauthor_extension_nodate,
	title = {Extension {API} — hls4ml 0.8.1 documentation},
	url = {https://fastmachinelearning.org/hls4ml/advanced/extension.html},
	urldate = {2024-01-15},
	file = {Extension API — hls4ml 0.8.1 documentation:C\:\\Users\\David\\Zotero\\storage\\FN3UNYHQ\\extension.html:text/html},
}

@misc{noauthor_release_nodate,
	title = {Release edelweiss 0.8.0 · fastmachinelearning/hls4ml},
	url = {https://github.com/fastmachinelearning/hls4ml/releases/tag/v0.8.0},
	abstract = {What's Changed

Decouple pipeline style from strategy by @vloncar in \#781
Don't use reader in ModelGraph and layers by @vloncar in \#770
Remove tf\_to\_hls by @vloncar in \#795
[pre-commit.ci] pre-comm...},
	language = {en},
	urldate = {2024-01-15},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\WA4CKTN2\\v0.8.html:text/html},
}

@misc{noauthor_welcome_nodate,
	title = {Welcome to hls4ml’s documentation! — hls4ml 0.8.1 documentation},
	url = {https://fastmachinelearning.org/hls4ml/},
	urldate = {2024-01-15},
	file = {Welcome to hls4ml’s documentation! — hls4ml 0.8.1 documentation:C\:\\Users\\David\\Zotero\\storage\\LD424CVZ\\hls4ml.html:text/html},
}

@misc{noauthor_jaccard_2023,
	title = {Jaccard index},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Jaccard_index&oldid=1188130957},
	abstract = {The Jaccard index, also known as the Jaccard similarity coefficient, is a statistic used for gauging the similarity and diversity of sample sets.
It was developed by Grove Karl Gilbert in 1884 as his ratio of verification (v) and now is frequently referred to as the Critical Success Index in meteorology. It was later developed independently by Paul Jaccard, originally giving the French name coefficient de communauté, and independently formulated again by T. Tanimoto. Thus, the Tanimoto index or Tanimoto coefficient are also used in some fields. However, they are identical in generally taking the ratio of Intersection over Union.},
	language = {en},
	urldate = {2024-01-15},
	journal = {Wikipedia},
	month = dec,
	year = {2023},
	note = {Page Version ID: 1188130957},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\GK26FCXC\\Jaccard_index.html:text/html},
}

@article{everingham_pascal_2010,
	title = {The {Pascal} {Visual} {Object} {Classes} ({VOC}) {Challenge}},
	volume = {88},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-009-0275-4},
	doi = {10.1007/s11263-009-0275-4},
	abstract = {The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.},
	language = {en},
	number = {2},
	urldate = {2024-01-15},
	journal = {International Journal of Computer Vision},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	month = jun,
	year = {2010},
	pages = {303--338},
	file = {Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:C\:\\Users\\David\\Zotero\\storage\\QHPUX5GL\\Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:application/pdf},
}

@misc{noauthor_coco_nodate,
	title = {{COCO} - {Common} {Objects} in {Context}},
	url = {https://cocodataset.org/#detection-eval},
	urldate = {2024-01-15},
	file = {COCO - Common Objects in Context:C\:\\Users\\David\\Zotero\\storage\\8W63QHSJ\\cocodataset.org.html:text/html},
}

@misc{noauthor_perceptron_3png_nodate,
	title = {perceptron\_3.png (1058×482)},
	url = {https://dv495y1g0kef5.cloudfront.net/training/data_scientist_airbnb/img/perceptron_3.png},
	urldate = {2024-01-15},
	file = {perceptron_3.png (1058×482):C\:\\Users\\David\\Zotero\\storage\\HLLV8V65\\perceptron_3.html:text/html},
}

@misc{noauthor_learning_rate_choicepng_nodate,
	title = {learning\_rate\_choice.png (1235×479)},
	url = {https://duchesnay.github.io/pystatsml/_images/learning_rate_choice.png},
	urldate = {2024-01-15},
	file = {learning_rate_choice.png (1235×479):C\:\\Users\\David\\Zotero\\storage\\7N5ZDB5E\\learning_rate_choice.html:text/html},
}

@misc{noauthor_wnixdpng_nodate,
	title = {{WNIXd}.png (3099×796)},
	url = {https://i.stack.imgur.com/WNIXd.png},
	urldate = {2024-01-15},
	file = {WNIXd.png (3099×796):C\:\\Users\\David\\Zotero\\storage\\BKVMPE2M\\WNIXd.html:text/html},
}

@misc{noauthor_figure_nodate,
	title = {{FIGURE} 2. {Example} of an {1D} signal (rendered in blue) convolved (stride...},
	url = {https://www.researchgate.net/figure/Example-of-an-1D-signal-rendered-in-blue-convolved-stride-s-3-with-a-kernel-of-size_fig1_339637429},
	abstract = {Download scientific diagram {\textbar} Example of an 1D signal (rendered in blue) convolved (stride s = 3) with a kernel of size k = 3 (in red). The positions in which the kernel is centered during the convolution process are rendered in dark blue. from publication: Hyperspectral Band Selection Using Attention-Based Convolutional Neural Networks {\textbar} Hyperspectral imaging has become a mature technology which brings exciting possibilities in various domains, including satellite image analysis. However, the high dimensionality and volume of such imagery is a serious problem which needs to be faced in Earth Observation... {\textbar} Convolution, Neural Networks and Classification {\textbar} ResearchGate, the professional network for scientists.},
	language = {en},
	urldate = {2024-01-15},
	journal = {ResearchGate},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\6XEJB5ZE\\Example-of-an-1D-signal-rendered-in-blue-convolved-stride-s-3-with-a-kernel-of-size_fig1_339637.html:text/html},
}

@misc{noauthor_06_09png_nodate,
	title = {06\_09.png (1854×807)},
	url = {https://media5.datahacker.rs/2018/11/06_09.png},
	urldate = {2024-01-15},
	file = {06_09.png (1854×807):C\:\\Users\\David\\Zotero\\storage\\DC49S2VZ\\06_09.html:text/html},
}

@article{mohammed_spatiotemporal_2024,
	title = {Spatiotemporal convolutional long short-term memory for regional streamflow predictions},
	volume = {350},
	issn = {0301-4797},
	url = {https://www.sciencedirect.com/science/article/pii/S0301479723023733},
	doi = {10.1016/j.jenvman.2023.119585},
	abstract = {Rainfall-runoff (RR) modelling is a challenging task in hydrology, especially at the regional scale. This work presents an approach to simultaneously predict daily streamflow in 86 catchments across the US using a sequential CNN-LSTM deep learning architecture. The model effectively incorporates both spatial and temporal information, leveraging the CNN to encode spatial patterns and the LSTM to learn their temporal relations. For training, a year-long spatially distributed input with precipitation, maximum temperature, and minimum temperature for each day was used to predict one-day streamflow. The trained CNN-LSTM model was further fine-tuned for three local sub-clusters of the 86 stations, assessing the significance of fine-tuning in model performance. The CNN-LSTM model, post fine-tuning, exhibited strong predictive capabilities with a median Nash-Sutcliffe efficiency (NSE) of 0.62 over the test period. Remarkably, 65\% of the 86 stations achieved NSE values greater than 0.6. The performance of the model was also compared to different deep learning models trained using a similar setup (CNN, LSTM, ANN). An LSTM model was also developed and trained individually to predict for each of the stations using local data. The CNN-LSTM model outperformed all the models which was trained regionally, and achieved a comparable performance to the local LSTM model. Fine-tuning improved the performance of all models during the test period. The results highlight the potential of the CNN-LSTM approach for regional RR modelling by effectively capturing complex spatiotemporal patterns inherent in the RR process.},
	urldate = {2024-01-15},
	journal = {Journal of Environmental Management},
	author = {Mohammed, Abdalla and Corzo, Gerald},
	month = jan,
	year = {2024},
	keywords = {CAMELS, CNN, Deep learning, LSTM, Rainfall-runoff, Regional modelling},
	pages = {119585},
	file = {ScienceDirect Snapshot:C\:\\Users\\David\\Zotero\\storage\\PQID23J4\\S0301479723023733.html:text/html},
}

@misc{noauthor_maxpoolsample2png_nodate,
	title = {{MaxpoolSample2}.png (750×313)},
	url = {https://production-media.paperswithcode.com/methods/MaxpoolSample2.png},
	urldate = {2024-01-15},
	file = {MaxpoolSample2.png (750×313):C\:\\Users\\David\\Zotero\\storage\\MFFCSL52\\MaxpoolSample2.html:text/html},
}

@misc{noauthor_figure_nodate-1,
	title = {Figure 3.2.: {The} {YOLO} {Detection} {System} [17 ]},
	shorttitle = {Figure 3.2.},
	url = {https://www.researchgate.net/figure/The-YOLO-Detection-System-17_fig5_340307540},
	abstract = {Download scientific diagram {\textbar} 2.: The YOLO Detection System [17 ] from publication: Evaluation of Machine Learning Algorithms for Object Detection in Technical Drawings like P\&IDs and Circuit Diagrams {\textbar} The need for digitization of complex engineering drawings has come into prominence for the industrial applications in order to reconstruct these diagrams and extract informa- tion efficiently. Up to the present, several different approaches have been demonstrated. Although... {\textbar} Object Detection, Technical Drawing and Digitization {\textbar} ResearchGate, the professional network for scientists.},
	language = {en},
	urldate = {2024-01-15},
	journal = {ResearchGate},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\26I3UUDR\\The-YOLO-Detection-System-17_fig5_340307540.html:text/html},
}

@misc{noauthor_figure_nodate-2,
	title = {Figure 37-{Modèle} utilisé par l'algorithme {YOLO} (de {Redmon}, {Divvala},...},
	url = {https://www.researchgate.net/figure/Modele-utilise-par-lalgorithme-YOLO-de-Redmon-Divvala-Girshick-Farhadi-2016_fig25_327882341},
	abstract = {Download scientific diagram {\textbar} Modèle utilisé par l'algorithme YOLO (de Redmon, Divvala, Girshick, \& Farhadi, 2016) from publication: Roger Federer vs. Deep Learning: can AI predict Federer's serve ? {\textbar} Deep Learning and Artificial Intelligence {\textbar} ResearchGate, the professional network for scientists.},
	language = {en},
	urldate = {2024-01-15},
	journal = {ResearchGate},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\U5CRTFXX\\Modele-utilise-par-lalgorithme-YOLO-de-Redmon-Divvala-Girshick-Farhadi-2016_fig25_327882341.html:text/html},
}

@misc{noauthor_figure_nodate-3,
	title = {Figure 3. {YOLO} network architecture (adapted from [44]).},
	url = {https://www.researchgate.net/figure/YOLO-network-architecture-adapted-from-44_fig1_330484322},
	abstract = {Download scientific diagram {\textbar} YOLO network architecture (adapted from [44]). from publication: Deep Neural Networks and Kernel Density Estimation for Detecting Human Activity Patterns from Geo-Tagged Images: A Case Study of Birdwatching on Flickr {\textbar} Thanks to recent advances in high-performance computing and deep learning, computer vision algorithms coupled with spatial analysis methods provide a unique opportunity for extracting human activity patterns from geo-tagged social media images. However, there are only a... {\textbar} Computer Vision, Neural Networks and Images {\textbar} ResearchGate, the professional network for scientists.},
	language = {en},
	urldate = {2024-01-15},
	journal = {ResearchGate},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\EYARAJTP\\YOLO-network-architecture-adapted-from-44_fig1_330484322.html:text/html},
}

@misc{noauthor_brief_nodate,
	title = {Brief summary of {YOLOv8} model structure · {Issue} \#189 · ultralytics/ultralytics},
	url = {https://github.com/ultralytics/ultralytics/issues/189},
	abstract = {Model structure of YOLOv8 detection models(P5) - yolov8n/s/m/l/x: Changes compared to YOLOv5: Replace the C3 module with the C2f module Replace the first 6x6 Conv with 3x3 Conv in the Backbone Dele...},
	language = {en},
	urldate = {2024-01-15},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\6NRT4XYE\\189.html:text/html},
}

@misc{noauthor_image_nodate,
	title = {Image size is changed when do model.predict · {Issue} \#3955 · ultralytics/ultralytics},
	url = {https://github.com/ultralytics/ultralytics/issues/3955},
	abstract = {Search before asking I have searched the YOLOv8 issues and discussions and found no similar questions. Question Hello, could you please provide me with some clarification? I trained a YOLOv8n model...},
	language = {en},
	urldate = {2024-01-15},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\HM2GEFW9\\3955.html:text/html},
}

@misc{thatsnotmyname71_what_2019,
	type = {Forum post},
	title = {What do the numbers in this {CNN} architecture stand for?},
	url = {https://ai.stackexchange.com/q/13842},
	urldate = {2024-01-15},
	journal = {Artificial Intelligence Stack Exchange},
	author = {thatsnotmyname71},
	month = aug,
	year = {2019},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\F3LSKRJA\\what-do-the-numbers-in-this-cnn-architecture-stand-for.html:text/html},
}

@misc{nbro_answer_2019,
	title = {Answer to "{What} do the numbers in this {CNN} architecture stand for?"},
	shorttitle = {Answer to "{What} do the numbers in this {CNN} architecture stand for?},
	url = {https://ai.stackexchange.com/a/13844},
	urldate = {2024-01-15},
	journal = {Artificial Intelligence Stack Exchange},
	author = {nbro},
	month = aug,
	year = {2019},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\RDEPGKES\\what-do-the-numbers-in-this-cnn-architecture-stand-for.html:text/html},
}

@misc{noauthor_alveo_nodate,
	title = {Alveo {U250} {Data} {Center} {Accelerator} {Card}},
	url = {https://www.xilinx.com/products/boards-and-kits/alveo/u250.html},
	abstract = {AMD U250 Alveo Data Center accelerator cards are designed to meet the constantly changing needs of the modern Data Center, providing up to 90X performance increase over CPUs for most common workloads, including machine learning inference, video transcoding, and database search \& analytics.},
	language = {en},
	urldate = {2024-01-15},
	journal = {AMD},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\EWCFKVM5\\u250.html:text/html},
}

@misc{jocher_yolov5_2020,
	title = {{YOLOv5} by {Ultralytics}},
	copyright = {AGPL-3.0},
	url = {https://github.com/ultralytics/yolov5},
	abstract = {YOLOv5 �� in PyTorch {\textgreater} ONNX {\textgreater} CoreML {\textgreater} TFLite},
	urldate = {2024-01-15},
	author = {Jocher, Glenn},
	month = may,
	year = {2020},
	doi = {10.5281/zenodo.3908559},
}

@misc{jocher_ultralytics_2023,
	title = {Ultralytics {YOLO}},
	copyright = {AGPL-3.0},
	url = {https://github.com/ultralytics/ultralytics},
	abstract = {NEW - YOLOv8 �� in PyTorch {\textgreater} ONNX {\textgreater} OpenVINO {\textgreater} CoreML {\textgreater} TFLite},
	urldate = {2024-01-15},
	author = {Jocher, Glenn and Chaurasia, Ayush and Qiu, Jing},
	month = jan,
	year = {2023},
	note = {original-date: 2022-09-11T16:39:45Z},
}


@article{lyons_discovering_2013,
	title = {Discovering the {Significance} of 5 sigma},
	url = {http://arxiv.org/abs/1310.1284},
	abstract = {We discuss the traditional criterion for discovery in Particle Physics of requiring a significance corresponding to at least 5 sigma; and whether a more nuanced approach might be better.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Lyons, Louis},
	month = oct,
	year = {2013},
	note = {arXiv:1310.1284 [hep-ex, physics:hep-ph, physics:physics]},
	keywords = {High Energy Physics - Experiment, High Energy Physics - Phenomenology, Physics - Data Analysis, Statistics and Probability},
	annote = {Comment: 7 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\4SPIXKIT\\1310.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\8BTHJ2LU\\Lyons - 2013 - Discovering the Significance of 5 sigma.pdf:application/pdf},
}

@misc{ultralytics_detect_nodate,
	title = {Detect},
	url = {https://docs.ultralytics.com/tasks/detect},
	abstract = {Official documentation for YOLOv8 by Ultralytics. Learn how to train, validate, predict and export models in various formats. Including detailed performance stats.},
	language = {en},
	urldate = {2024-01-15},
	author = {Ultralytics},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\WRXAREAC\\detect.html:text/html},
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\N4GT6WPW\\1412.html:text/html;Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\RYV729D7\\Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf},
}

@misc{noauthor_introduction_nodate,
	title = {Introduction to triggers},
	url = {https://atlassoftwaredocs.web.cern.ch/AnalysisSWTutorial/trig_intro/},
	abstract = {Introduction to triggers},
	urldate = {2024-01-15},
	journal = {ATLAS Software Documentation},
	file = {Snapshot:C\:\\Users\\David\\Zotero\\storage\\IQHNQTM6\\trig_intro.html:text/html},
}

@misc{subramanyam_iouintersection_2021,
	title = {{IOU}({Intersection} over {Union})},
	url = {https://medium.com/analytics-vidhya/iou-intersection-over-union-705a39e7acef},
	abstract = {What is IOU?},
	language = {en},
	urldate = {2024-01-16},
	journal = {Analytics Vidhya},
	author = {Subramanyam, Vineeth S.},
	month = jan,
	year = {2021},
	file = {Snapshot:/home/david/Zotero/storage/U9QH5VA8/iou-intersection-over-union-705a39e7acef.html:text/html},
}

@techreport{karimi_confusion_2021,
	title = {Confusion {Matrix}},
	abstract = {The confusion matrix is a tool for predictive analysis In machine learning. In order to check the performance of a classification based machine learning model, the confusion matix is deployed.},
	author = {Karimi, Zohreh},
	month = oct,
	year = {2021},
	file = {Full Text PDF:/home/david/Zotero/storage/5JUSRN2I/Karimi - 2021 - Confusion Matrix.pdf:application/pdf},
}

@misc{padilla_rafaelpadillaobject-detection-metrics_2024,
	title = {rafaelpadilla/{Object}-{Detection}-{Metrics}},
	copyright = {MIT},
	url = {https://github.com/rafaelpadilla/Object-Detection-Metrics},
	abstract = {Most popular metrics used to evaluate object detection algorithms.},
	urldate = {2024-01-16},
	author = {Padilla, Rafael},
	month = jan,
	year = {2024},
	note = {original-date: 2018-05-23T17:51:15Z},
	keywords = {average-precision, bounding-boxes, mean-average-precision, metrics, object-detection, pascal-voc, precision-recall},
}

@article{fleuret_deep_nodate-4,
	title = {Deep learning 3.1. {The} perceptron},
	language = {en},
	journal = {Deep learning},
	author = {Fleuret, Francois},
	file = {Fleuret - Deep learning 3.1. The perceptron.pdf:/home/david/Zotero/storage/L4YE4FXH/Fleuret - Deep learning 3.1. The perceptron.pdf:application/pdf},
}
